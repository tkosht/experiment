{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from app.auto_topic.component.models.model import TextSequences\n",
    "from app.auto_topic.component.models.pipeline import Pipeline\n",
    "from app.auto_topic.component.models.vectorizer import VectorizerBoW, VectorizerWord2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDでもっと便利に', 'みんなの意見', '爆発物の情報中部空港に緊急着陸', '広島サミット韓国の招待を検討', 'エーザイ認知症薬米FDAが承認']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_sentences = joblib.load(\"tidy_sentences.gz\")\n",
    "tidy_sentences[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[(JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True), None), (VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5), None)], name=pipeline, do_print=True, args=(), kwargs={'args': (), 'kwargs': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe_wikivec = joblib.load(\"data/pipe_wikivec.noun-verb.gz\")\n",
    "pipe_wikivec = joblib.load(\"data/pipe_wikivec.gz\")\n",
    "pipe_wikivec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000\n",
      "2023/02/12 20:22:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:22:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:22:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:22:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "10000 20000\n",
      "2023/02/12 20:22:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:22:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:22:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:22:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "20000 30000\n",
      "2023/02/12 20:22:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:22:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:23:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:23:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:23:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "30000 40000\n",
      "2023/02/12 20:23:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:23:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:23:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:23:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "40000 50000\n",
      "2023/02/12 20:23:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:23:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:24:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:24:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:24:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "50000 60000\n",
      "2023/02/12 20:24:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:24:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:24:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:24:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "60000 70000\n",
      "2023/02/12 20:24:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:24:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:24:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:24:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "70000 80000\n",
      "2023/02/12 20:24:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:24:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:25:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:25:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:25:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "80000 90000\n",
      "2023/02/12 20:25:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:25:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:25:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:25:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "90000 100000\n",
      "2023/02/12 20:25:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:25:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:26:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:26:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:26:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "100000 110000\n",
      "2023/02/12 20:26:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:26:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:26:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:26:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "110000 120000\n",
      "2023/02/12 20:26:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:26:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:26:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:26:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "120000 130000\n",
      "2023/02/12 20:26:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:26:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:27:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:27:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:27:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "130000 140000\n",
      "2023/02/12 20:27:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:27:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:27:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:27:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "140000 150000\n",
      "2023/02/12 20:27:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:27:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:27:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:27:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "150000 160000\n",
      "2023/02/12 20:27:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:27:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:28:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:28:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:28:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "160000 170000\n",
      "2023/02/12 20:28:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:28:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:28:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:28:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "170000 180000\n",
      "2023/02/12 20:28:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:28:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:29:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:29:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:29:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "180000 190000\n",
      "2023/02/12 20:29:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:29:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:29:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:29:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "190000 200000\n",
      "2023/02/12 20:29:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:29:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:29:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:29:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "200000 210000\n",
      "2023/02/12 20:29:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:29:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:30:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:30:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:30:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "210000 220000\n",
      "2023/02/12 20:30:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:30:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:30:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:30:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "220000 230000\n",
      "2023/02/12 20:30:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:30:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:30:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:30:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "230000 240000\n",
      "2023/02/12 20:30:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:30:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:31:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:31:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:31:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "240000 250000\n",
      "2023/02/12 20:31:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:20 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:20 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:31:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:31:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:31:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "250000 260000\n",
      "2023/02/12 20:31:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:31:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:32:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:32:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:32:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "260000 270000\n",
      "2023/02/12 20:32:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:32:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:32:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:32:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "270000 280000\n",
      "2023/02/12 20:32:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:32:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:32:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:32:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "280000 290000\n",
      "2023/02/12 20:32:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:32:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:33:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:33:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:33:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "290000 300000\n",
      "2023/02/12 20:33:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:33:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:33:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:33:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "300000 310000\n",
      "2023/02/12 20:33:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:33:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:33:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:33:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "310000 320000\n",
      "2023/02/12 20:33:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:33:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:34:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:34:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:34:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "320000 330000\n",
      "2023/02/12 20:34:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:20 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:20 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:34:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:34:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:34:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "330000 340000\n",
      "2023/02/12 20:34:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:34:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:35:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:35:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:35:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "340000 350000\n",
      "2023/02/12 20:35:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:35:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:35:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:35:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "350000 360000\n",
      "2023/02/12 20:35:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:35:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:35:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:35:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "360000 370000\n",
      "2023/02/12 20:35:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:35:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:36:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:36:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:36:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "370000 380000\n",
      "2023/02/12 20:36:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:36:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:36:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:36:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "380000 390000\n",
      "2023/02/12 20:36:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:36:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:36:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:36:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:37:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "390000 400000\n",
      "2023/02/12 20:37:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:37:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:37:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:37:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "400000 410000\n",
      "2023/02/12 20:37:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:37:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:37:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:37:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "410000 420000\n",
      "2023/02/12 20:37:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:37:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:38:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:38:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:38:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "420000 430000\n",
      "2023/02/12 20:38:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:38:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:38:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:38:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "430000 440000\n",
      "2023/02/12 20:38:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:38:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:38:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:38:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "440000 450000\n",
      "2023/02/12 20:38:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:38:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:39:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:39:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:39:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "450000 460000\n",
      "2023/02/12 20:39:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:39:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:39:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:39:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "460000 470000\n",
      "2023/02/12 20:39:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:39:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:40:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:40:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:40:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "470000 480000\n",
      "2023/02/12 20:40:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:40:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:40:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:40:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "480000 490000\n",
      "2023/02/12 20:40:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:40:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:40:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:40:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "490000 500000\n",
      "2023/02/12 20:40:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:40:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:41:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:41:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:41:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "500000 510000\n",
      "2023/02/12 20:41:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:41:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:41:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:41:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "510000 520000\n",
      "2023/02/12 20:41:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:41:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:41:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:41:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "520000 530000\n",
      "2023/02/12 20:41:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:41:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:42:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:42:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:42:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "530000 540000\n",
      "2023/02/12 20:42:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:42:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:42:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:42:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "540000 550000\n",
      "2023/02/12 20:42:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:42:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:42:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:42:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "550000 560000\n",
      "2023/02/12 20:42:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:42:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:43:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:43:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:43:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "560000 570000\n",
      "2023/02/12 20:43:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:43:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:43:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:43:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "570000 580000\n",
      "2023/02/12 20:43:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:43:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:44:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:44:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:44:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "580000 590000\n",
      "2023/02/12 20:44:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:44:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:44:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:44:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "590000 600000\n",
      "2023/02/12 20:44:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:44:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:44:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:44:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "600000 610000\n",
      "2023/02/12 20:44:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:44:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:45:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:45:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:45:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "610000 620000\n",
      "2023/02/12 20:45:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:45:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:45:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:45:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "620000 630000\n",
      "2023/02/12 20:45:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:45:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:46:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:46:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:46:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "630000 640000\n",
      "2023/02/12 20:46:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:46:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:46:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:46:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "640000 650000\n",
      "2023/02/12 20:46:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:46:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:46:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:46:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "650000 660000\n",
      "2023/02/12 20:46:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:46:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:47:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:47:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:47:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "660000 670000\n",
      "2023/02/12 20:47:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:47:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:47:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:47:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "670000 680000\n",
      "2023/02/12 20:47:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:47:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:47:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:47:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "680000 690000\n",
      "2023/02/12 20:47:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:47:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:48:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:48:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:48:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "690000 700000\n",
      "2023/02/12 20:48:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:48:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:48:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:48:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "700000 710000\n",
      "2023/02/12 20:48:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:48:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:48:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:48:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "710000 720000\n",
      "2023/02/12 20:48:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:48:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:49:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:49:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:49:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "720000 730000\n",
      "2023/02/12 20:49:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:49:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:49:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:49:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "730000 740000\n",
      "2023/02/12 20:49:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:49:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:50:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:50:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:50:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "740000 750000\n",
      "2023/02/12 20:50:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:50:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:50:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:50:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "750000 760000\n",
      "2023/02/12 20:50:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:50:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:50:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:50:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "760000 770000\n",
      "2023/02/12 20:50:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:50:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:51:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:51:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:51:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "770000 780000\n",
      "2023/02/12 20:51:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:51:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:51:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:51:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "780000 790000\n",
      "2023/02/12 20:51:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:51:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:51:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:51:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "790000 800000\n",
      "2023/02/12 20:51:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:51:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:52:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:52:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:52:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "800000 810000\n",
      "2023/02/12 20:52:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:52:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:52:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:52:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "810000 820000\n",
      "2023/02/12 20:52:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:52:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:53:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:53:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:53:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "820000 830000\n",
      "2023/02/12 20:53:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:53:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:53:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:53:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "830000 840000\n",
      "2023/02/12 20:53:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:53:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:53:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:53:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "840000 850000\n",
      "2023/02/12 20:53:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:53:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:54:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:54:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:54:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "850000 860000\n",
      "2023/02/12 20:54:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:54:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:54:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:54:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "860000 870000\n",
      "2023/02/12 20:54:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:54:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:54:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:54:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "870000 880000\n",
      "2023/02/12 20:54:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:54:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:55:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:55:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:55:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "880000 890000\n",
      "2023/02/12 20:55:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:55:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:55:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:55:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "890000 900000\n",
      "2023/02/12 20:55:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:55:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:56:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:56:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:56:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "900000 910000\n",
      "2023/02/12 20:56:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:56:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:56:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:56:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "910000 920000\n",
      "2023/02/12 20:56:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:56:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:56:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:56:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "920000 930000\n",
      "2023/02/12 20:56:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:56:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:57:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:57:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:57:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "930000 940000\n",
      "2023/02/12 20:57:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:57:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:57:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:57:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "940000 950000\n",
      "2023/02/12 20:57:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:57:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:58:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:58:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:58:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "950000 960000\n",
      "2023/02/12 20:58:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:58:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:58:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:58:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "960000 970000\n",
      "2023/02/12 20:58:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:58:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:58:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:58:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "970000 980000\n",
      "2023/02/12 20:58:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:58:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:59:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:59:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:59:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "980000 990000\n",
      "2023/02/12 20:59:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:59:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:59:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:59:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "990000 1000000\n",
      "2023/02/12 20:59:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 20:59:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:59:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 20:59:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1000000 1010000\n",
      "2023/02/12 20:59:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 20:59:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:00:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:00:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:00:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1010000 1020000\n",
      "2023/02/12 21:00:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:00:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:00:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:00:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1020000 1030000\n",
      "2023/02/12 21:00:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:00:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:01:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:01:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:01:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1030000 1040000\n",
      "2023/02/12 21:01:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:01:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:01:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:01:25 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1040000 1050000\n",
      "2023/02/12 21:01:25 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:25 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:25 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:25 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:25 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:01:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:01:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:01:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1050000 1060000\n",
      "2023/02/12 21:01:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:01:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:02:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:02:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:02:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1060000 1070000\n",
      "2023/02/12 21:02:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:02:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:02:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:02:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1070000 1080000\n",
      "2023/02/12 21:02:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:02:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:02:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:02:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1080000 1090000\n",
      "2023/02/12 21:02:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:02:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:03:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:03:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:03:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1090000 1100000\n",
      "2023/02/12 21:03:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:03:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:03:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:03:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1100000 1110000\n",
      "2023/02/12 21:03:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:03:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:04:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:04:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:04:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1110000 1120000\n",
      "2023/02/12 21:04:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:04:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:04:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:04:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1120000 1130000\n",
      "2023/02/12 21:04:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:04:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:04:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:04:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1130000 1140000\n",
      "2023/02/12 21:04:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:04:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:05:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:05:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:05:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1140000 1150000\n",
      "2023/02/12 21:05:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:05:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:05:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:05:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1150000 1160000\n",
      "2023/02/12 21:05:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:05:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:06:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:06:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:06:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1160000 1170000\n",
      "2023/02/12 21:06:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:06:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:06:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:06:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1170000 1180000\n",
      "2023/02/12 21:06:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:06:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:06:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:06:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1180000 1190000\n",
      "2023/02/12 21:06:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:06:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:07:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:07:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:07:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1190000 1200000\n",
      "2023/02/12 21:07:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:07:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:07:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:07:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1200000 1210000\n",
      "2023/02/12 21:07:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:07:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:07:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:07:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1210000 1220000\n",
      "2023/02/12 21:07:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:07:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:08:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:08:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:08:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1220000 1230000\n",
      "2023/02/12 21:08:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:08:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:08:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:08:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1230000 1240000\n",
      "2023/02/12 21:08:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:08:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:09:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:09:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:09:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1240000 1250000\n",
      "2023/02/12 21:09:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:09:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:09:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:09:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1250000 1260000\n",
      "2023/02/12 21:09:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:09:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:09:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:09:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1260000 1270000\n",
      "2023/02/12 21:09:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:09:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:10:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:10:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:10:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1270000 1280000\n",
      "2023/02/12 21:10:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:10:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:10:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:10:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1280000 1290000\n",
      "2023/02/12 21:10:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:10:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:10:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:10:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1290000 1300000\n",
      "2023/02/12 21:10:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:10:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:11:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:11:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:11:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1300000 1310000\n",
      "2023/02/12 21:11:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:11:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:11:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:11:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1310000 1320000\n",
      "2023/02/12 21:11:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:11:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:12:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:12:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:12:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1320000 1330000\n",
      "2023/02/12 21:12:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:12:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:12:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:12:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1330000 1340000\n",
      "2023/02/12 21:12:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:12:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:12:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:12:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1340000 1350000\n",
      "2023/02/12 21:12:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:12:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:13:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:13:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:13:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1350000 1360000\n",
      "2023/02/12 21:13:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:13:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:13:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:13:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1360000 1370000\n",
      "2023/02/12 21:13:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:13:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:13:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:13:57 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1370000 1380000\n",
      "2023/02/12 21:13:57 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:57 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:57 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:13:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:14:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:14:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:14:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1380000 1390000\n",
      "2023/02/12 21:14:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:20 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:20 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:14:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:14:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:14:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1390000 1400000\n",
      "2023/02/12 21:14:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:14:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:15:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:15:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:15:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1400000 1410000\n",
      "2023/02/12 21:15:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:15:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:15:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:15:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1410000 1420000\n",
      "2023/02/12 21:15:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:15:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:15:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:15:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1420000 1430000\n",
      "2023/02/12 21:15:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:15:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:16:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:16:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:16:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1430000 1440000\n",
      "2023/02/12 21:16:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:16:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:16:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:16:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1440000 1450000\n",
      "2023/02/12 21:16:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:16:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:16:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:16:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1450000 1460000\n",
      "2023/02/12 21:16:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:16:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:17:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:17:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:17:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1460000 1470000\n",
      "2023/02/12 21:17:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:17:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:17:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:17:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1470000 1480000\n",
      "2023/02/12 21:17:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:17:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:18:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:18:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:18:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1480000 1490000\n",
      "2023/02/12 21:18:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:18:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:18:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:18:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1490000 1500000\n",
      "2023/02/12 21:18:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:18:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:18:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:18:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1500000 1510000\n",
      "2023/02/12 21:18:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:18:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:19:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:19:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:19:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1510000 1520000\n",
      "2023/02/12 21:19:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:19:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:19:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:19:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1520000 1530000\n",
      "2023/02/12 21:19:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:19:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:19:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:19:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1530000 1540000\n",
      "2023/02/12 21:19:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:19:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:20:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:20:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:20:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1540000 1550000\n",
      "2023/02/12 21:20:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:20:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:20:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:20:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1550000 1560000\n",
      "2023/02/12 21:20:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:20:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:21:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:21:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:21:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1560000 1570000\n",
      "2023/02/12 21:21:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:21:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:21:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:21:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1570000 1580000\n",
      "2023/02/12 21:21:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:21:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:21:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:21:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1580000 1590000\n",
      "2023/02/12 21:21:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:21:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:22:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:22:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:22:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1590000 1600000\n",
      "2023/02/12 21:22:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:22:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:22:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:22:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1600000 1610000\n",
      "2023/02/12 21:22:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:22:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:23:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:23:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:23:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1610000 1620000\n",
      "2023/02/12 21:23:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:23:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:23:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:23:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1620000 1630000\n",
      "2023/02/12 21:23:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:23:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:23:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:23:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1630000 1640000\n",
      "2023/02/12 21:23:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:23:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:24:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:24:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:24:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1640000 1650000\n",
      "2023/02/12 21:24:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:24:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:24:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:24:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1650000 1660000\n",
      "2023/02/12 21:24:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:24:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:24:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:24:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1660000 1670000\n",
      "2023/02/12 21:24:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:24:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:25:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:25:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:25:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1670000 1680000\n",
      "2023/02/12 21:25:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:25:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:25:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:25:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1680000 1690000\n",
      "2023/02/12 21:25:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:25:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:26:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:26:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:26:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1690000 1700000\n",
      "2023/02/12 21:26:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:26:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:26:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:26:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1700000 1710000\n",
      "2023/02/12 21:26:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:26:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:26:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:26:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1710000 1720000\n",
      "2023/02/12 21:26:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:26:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:27:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:27:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:27:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1720000 1730000\n",
      "2023/02/12 21:27:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:27:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:27:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:27:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1730000 1740000\n",
      "2023/02/12 21:27:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:27:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:27:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:27:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1740000 1750000\n",
      "2023/02/12 21:27:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:27:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:28:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:28:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:28:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1750000 1760000\n",
      "2023/02/12 21:28:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:28:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:28:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:28:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1760000 1770000\n",
      "2023/02/12 21:28:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:28:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:29:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:29:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:29:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1770000 1780000\n",
      "2023/02/12 21:29:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:29:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:29:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:29:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1780000 1790000\n",
      "2023/02/12 21:29:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:29:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:29:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:29:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1790000 1800000\n",
      "2023/02/12 21:29:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:29:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:30:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:30:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:30:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1800000 1810000\n",
      "2023/02/12 21:30:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:30:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:30:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:30:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1810000 1820000\n",
      "2023/02/12 21:30:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:30:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:30:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:30:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1820000 1830000\n",
      "2023/02/12 21:30:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:30:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:31:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:31:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:31:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1830000 1840000\n",
      "2023/02/12 21:31:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:31:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:31:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:31:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1840000 1850000\n",
      "2023/02/12 21:31:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:31:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:32:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:32:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:32:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1850000 1860000\n",
      "2023/02/12 21:32:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:32:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:32:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:32:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1860000 1870000\n",
      "2023/02/12 21:32:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:32:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:32:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:32:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1870000 1880000\n",
      "2023/02/12 21:32:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:32:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:33:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:33:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:33:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1880000 1890000\n",
      "2023/02/12 21:33:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:33:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:33:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:33:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1890000 1900000\n",
      "2023/02/12 21:33:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:33:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:33:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:33:57 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1900000 1910000\n",
      "2023/02/12 21:33:57 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:57 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:57 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:33:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:34:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:34:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:34:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1910000 1920000\n",
      "2023/02/12 21:34:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:34:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:34:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:34:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1920000 1930000\n",
      "2023/02/12 21:34:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:34:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:35:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:35:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:35:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1930000 1940000\n",
      "2023/02/12 21:35:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:35:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:35:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:35:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1940000 1950000\n",
      "2023/02/12 21:35:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:35:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:35:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:35:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1950000 1960000\n",
      "2023/02/12 21:35:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:35:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:36:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:36:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:36:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1960000 1970000\n",
      "2023/02/12 21:36:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:36:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:36:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:36:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1970000 1980000\n",
      "2023/02/12 21:36:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:36:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:36:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:36:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1980000 1990000\n",
      "2023/02/12 21:36:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:36:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:37:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:37:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:37:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "1990000 2000000\n",
      "2023/02/12 21:37:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:20 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:20 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:37:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:37:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:37:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2000000 2010000\n",
      "2023/02/12 21:37:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:37:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:38:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:38:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:38:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2010000 2020000\n",
      "2023/02/12 21:38:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:38:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:38:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:38:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2020000 2030000\n",
      "2023/02/12 21:38:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:38:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:38:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:38:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2030000 2040000\n",
      "2023/02/12 21:38:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:38:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:39:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:39:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:39:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2040000 2050000\n",
      "2023/02/12 21:39:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:39:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:39:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:39:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2050000 2060000\n",
      "2023/02/12 21:39:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:39:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:39:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:39:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2060000 2070000\n",
      "2023/02/12 21:39:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:39:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:40:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:40:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:40:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2070000 2080000\n",
      "2023/02/12 21:40:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:40:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:40:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:40:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2080000 2090000\n",
      "2023/02/12 21:40:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:40:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:41:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:41:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:41:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2090000 2100000\n",
      "2023/02/12 21:41:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:41:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:41:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:41:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2100000 2110000\n",
      "2023/02/12 21:41:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:41:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:41:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:41:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2110000 2120000\n",
      "2023/02/12 21:41:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:41:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:42:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:42:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:42:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2120000 2130000\n",
      "2023/02/12 21:42:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:42:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:42:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:42:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2130000 2140000\n",
      "2023/02/12 21:42:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:42:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:42:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:42:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2140000 2150000\n",
      "2023/02/12 21:42:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:42:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:43:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:43:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:43:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2150000 2160000\n",
      "2023/02/12 21:43:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:43:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:43:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:43:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2160000 2170000\n",
      "2023/02/12 21:43:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:43:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:44:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:44:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:44:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2170000 2180000\n",
      "2023/02/12 21:44:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:44:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:44:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:44:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2180000 2190000\n",
      "2023/02/12 21:44:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:44:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:44:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:44:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2190000 2200000\n",
      "2023/02/12 21:44:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:44:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:45:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:45:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:45:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2200000 2210000\n",
      "2023/02/12 21:45:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:45:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:45:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:45:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2210000 2220000\n",
      "2023/02/12 21:45:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:45:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:46:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:46:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:46:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2220000 2230000\n",
      "2023/02/12 21:46:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:46:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:46:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:46:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2230000 2240000\n",
      "2023/02/12 21:46:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:46:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:46:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:46:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2240000 2250000\n",
      "2023/02/12 21:46:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:46:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:47:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:47:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:47:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2250000 2260000\n",
      "2023/02/12 21:47:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:47:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:47:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:47:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2260000 2270000\n",
      "2023/02/12 21:47:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:47:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:47:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:47:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2270000 2280000\n",
      "2023/02/12 21:47:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:47:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:48:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:48:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:48:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2280000 2290000\n",
      "2023/02/12 21:48:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:48:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:48:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:48:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2290000 2300000\n",
      "2023/02/12 21:48:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:48:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:49:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:49:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:49:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2300000 2310000\n",
      "2023/02/12 21:49:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:49:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:49:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:49:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2310000 2320000\n",
      "2023/02/12 21:49:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:49:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:49:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:49:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2320000 2330000\n",
      "2023/02/12 21:49:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:49:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:50:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:50:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:50:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2330000 2340000\n",
      "2023/02/12 21:50:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:50:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:50:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:50:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2340000 2350000\n",
      "2023/02/12 21:50:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:50:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:50:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:50:57 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2350000 2360000\n",
      "2023/02/12 21:50:57 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:57 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:57 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:50:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:51:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:51:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:51:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2360000 2370000\n",
      "2023/02/12 21:51:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:20 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:20 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:51:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:51:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:51:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2370000 2380000\n",
      "2023/02/12 21:51:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:51:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:52:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:52:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:52:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2380000 2390000\n",
      "2023/02/12 21:52:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:52:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:52:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:52:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2390000 2400000\n",
      "2023/02/12 21:52:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:52:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:52:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:52:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2400000 2410000\n",
      "2023/02/12 21:52:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:52:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:53:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:53:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:53:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2410000 2420000\n",
      "2023/02/12 21:53:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:53:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:53:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:53:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2420000 2430000\n",
      "2023/02/12 21:53:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:53:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:53:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:53:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2430000 2440000\n",
      "2023/02/12 21:53:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:53:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:54:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:54:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:54:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2440000 2450000\n",
      "2023/02/12 21:54:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:54:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:54:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:54:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2450000 2460000\n",
      "2023/02/12 21:54:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:54:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:55:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:55:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:55:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2460000 2470000\n",
      "2023/02/12 21:55:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:55:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:55:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:55:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2470000 2480000\n",
      "2023/02/12 21:55:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:55:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:55:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:55:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2480000 2490000\n",
      "2023/02/12 21:55:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:55:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:56:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:56:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:56:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2490000 2500000\n",
      "2023/02/12 21:56:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:56:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:56:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:56:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2500000 2510000\n",
      "2023/02/12 21:56:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:56:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:57:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:57:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:57:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2510000 2520000\n",
      "2023/02/12 21:57:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:57:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:57:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:57:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2520000 2530000\n",
      "2023/02/12 21:57:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:57:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:57:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:57:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2530000 2540000\n",
      "2023/02/12 21:57:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:57:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:58:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:58:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:58:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2540000 2550000\n",
      "2023/02/12 21:58:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:58:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:58:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:58:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2550000 2560000\n",
      "2023/02/12 21:58:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:58:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:58:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:58:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2560000 2570000\n",
      "2023/02/12 21:58:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:58:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:59:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:59:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:59:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2570000 2580000\n",
      "2023/02/12 21:59:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 21:59:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:59:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 21:59:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2580000 2590000\n",
      "2023/02/12 21:59:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 21:59:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:00:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:00:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:00:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2590000 2600000\n",
      "2023/02/12 22:00:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:00:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:00:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:00:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2600000 2610000\n",
      "2023/02/12 22:00:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:00:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:00:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:00:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2610000 2620000\n",
      "2023/02/12 22:00:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:00:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:01:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:01:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:01:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2620000 2630000\n",
      "2023/02/12 22:01:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:01:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:01:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:01:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2630000 2640000\n",
      "2023/02/12 22:01:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:01:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:01:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:01:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2640000 2650000\n",
      "2023/02/12 22:01:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:01:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:02:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:02:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:02:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2650000 2660000\n",
      "2023/02/12 22:02:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:02:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:02:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:02:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2660000 2670000\n",
      "2023/02/12 22:02:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:02:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:03:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:03:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:03:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2670000 2680000\n",
      "2023/02/12 22:03:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:03:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:03:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:03:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2680000 2690000\n",
      "2023/02/12 22:03:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:03:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:03:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:03:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2690000 2700000\n",
      "2023/02/12 22:03:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:03:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:04:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:04:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:04:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2700000 2710000\n",
      "2023/02/12 22:04:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:04:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:04:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:04:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2710000 2720000\n",
      "2023/02/12 22:04:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:04:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:04:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:04:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2720000 2730000\n",
      "2023/02/12 22:04:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:04:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:05:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:05:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:05:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2730000 2740000\n",
      "2023/02/12 22:05:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:05:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:05:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:05:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2740000 2750000\n",
      "2023/02/12 22:05:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:05:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:06:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:06:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:06:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2750000 2760000\n",
      "2023/02/12 22:06:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:06:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:06:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:06:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2760000 2770000\n",
      "2023/02/12 22:06:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:06:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:06:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:06:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2770000 2780000\n",
      "2023/02/12 22:06:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:06:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:07:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:07:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:07:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2780000 2790000\n",
      "2023/02/12 22:07:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:07:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:07:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:07:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2790000 2800000\n",
      "2023/02/12 22:07:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:07:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:08:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:08:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:08:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2800000 2810000\n",
      "2023/02/12 22:08:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:08:25 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:08:25 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:08:25 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2810000 2820000\n",
      "2023/02/12 22:08:25 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:25 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:25 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:25 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:25 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:08:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:08:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:08:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2820000 2830000\n",
      "2023/02/12 22:08:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:48 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:08:48 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:09:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:09:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:09:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2830000 2840000\n",
      "2023/02/12 22:09:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:09:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:09:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:09:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2840000 2850000\n",
      "2023/02/12 22:09:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:09:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:09:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:09:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2850000 2860000\n",
      "2023/02/12 22:09:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:09:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:10:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:10:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:10:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2860000 2870000\n",
      "2023/02/12 22:10:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:10:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:10:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:10:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2870000 2880000\n",
      "2023/02/12 22:10:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:10:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:11:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:11:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:11:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2880000 2890000\n",
      "2023/02/12 22:11:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:11:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:11:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:11:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2890000 2900000\n",
      "2023/02/12 22:11:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:11:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:11:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:11:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2900000 2910000\n",
      "2023/02/12 22:11:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:11:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:12:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:12:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:12:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2910000 2920000\n",
      "2023/02/12 22:12:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:12:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:12:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:12:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2920000 2930000\n",
      "2023/02/12 22:12:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:12:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:12:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:12:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2930000 2940000\n",
      "2023/02/12 22:12:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:12:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:13:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:13:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:13:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2940000 2950000\n",
      "2023/02/12 22:13:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:13:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:13:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:13:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2950000 2960000\n",
      "2023/02/12 22:13:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:13:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:14:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:14:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:14:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2960000 2970000\n",
      "2023/02/12 22:14:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:14:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:14:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:14:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2970000 2980000\n",
      "2023/02/12 22:14:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:14:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:14:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:14:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2980000 2990000\n",
      "2023/02/12 22:14:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:14:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:15:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:15:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:15:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2990000 3000000\n",
      "2023/02/12 22:15:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:15:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:15:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:15:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3000000 3010000\n",
      "2023/02/12 22:15:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:15:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:15:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:15:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3010000 3020000\n",
      "2023/02/12 22:15:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:15:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:16:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:16:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:16:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3020000 3030000\n",
      "2023/02/12 22:16:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:16:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:16:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:16:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3030000 3040000\n",
      "2023/02/12 22:16:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:16:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:17:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:17:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:17:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3040000 3050000\n",
      "2023/02/12 22:17:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:17:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:17:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:17:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3050000 3060000\n",
      "2023/02/12 22:17:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:17:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:17:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:17:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3060000 3070000\n",
      "2023/02/12 22:17:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:17:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:18:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:18:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:18:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3070000 3080000\n",
      "2023/02/12 22:18:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:18:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:18:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:18:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3080000 3090000\n",
      "2023/02/12 22:18:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:18:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:18:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:18:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3090000 3100000\n",
      "2023/02/12 22:18:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:18:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:19:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:19:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:19:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3100000 3110000\n",
      "2023/02/12 22:19:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:19:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:19:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:19:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3110000 3120000\n",
      "2023/02/12 22:19:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:19:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:20:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:20:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:20:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3120000 3130000\n",
      "2023/02/12 22:20:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:20:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:20:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:20:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3130000 3140000\n",
      "2023/02/12 22:20:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:20:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:20:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:20:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3140000 3150000\n",
      "2023/02/12 22:20:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:48 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:20:48 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:21:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:21:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:21:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3150000 3160000\n",
      "2023/02/12 22:21:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:21:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:21:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:21:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3160000 3170000\n",
      "2023/02/12 22:21:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:21:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:21:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:21:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3170000 3180000\n",
      "2023/02/12 22:21:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:21:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:22:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:22:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:22:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3180000 3190000\n",
      "2023/02/12 22:22:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:22:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:22:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:22:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3190000 3200000\n",
      "2023/02/12 22:22:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:22:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:23:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:23:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:23:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3200000 3210000\n",
      "2023/02/12 22:23:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:23:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:23:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:23:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3210000 3220000\n",
      "2023/02/12 22:23:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:23:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:23:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:23:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3220000 3230000\n",
      "2023/02/12 22:23:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:23:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:24:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:24:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:24:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3230000 3240000\n",
      "2023/02/12 22:24:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:24:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:24:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:24:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3240000 3250000\n",
      "2023/02/12 22:24:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:24:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:24:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:24:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3250000 3260000\n",
      "2023/02/12 22:24:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:24:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:25:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:25:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:25:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3260000 3270000\n",
      "2023/02/12 22:25:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:20 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:20 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:25:43 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:25:43 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:25:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3270000 3280000\n",
      "2023/02/12 22:25:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:25:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:26:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:26:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:26:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3280000 3290000\n",
      "2023/02/12 22:26:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:26:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:26:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:26:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3290000 3300000\n",
      "2023/02/12 22:26:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:26:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:26:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:26:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3300000 3310000\n",
      "2023/02/12 22:26:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:26:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:27:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:27:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:27:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3310000 3320000\n",
      "2023/02/12 22:27:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:27:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:27:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:27:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3320000 3330000\n",
      "2023/02/12 22:27:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:27:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:28:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:28:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:28:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3330000 3340000\n",
      "2023/02/12 22:28:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:28:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:28:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:28:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3340000 3350000\n",
      "2023/02/12 22:28:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:28:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:28:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:28:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3350000 3360000\n",
      "2023/02/12 22:28:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:28:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:29:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:29:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:29:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3360000 3370000\n",
      "2023/02/12 22:29:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:29:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:29:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:29:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3370000 3380000\n",
      "2023/02/12 22:29:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:29:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:29:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:29:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3380000 3390000\n",
      "2023/02/12 22:29:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:29:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:30:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:30:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:30:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3390000 3400000\n",
      "2023/02/12 22:30:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:30:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:30:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:30:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3400000 3410000\n",
      "2023/02/12 22:30:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:30:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:31:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:31:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:31:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3410000 3420000\n",
      "2023/02/12 22:31:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:31:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:31:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:31:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3420000 3430000\n",
      "2023/02/12 22:31:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:31:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:31:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:31:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3430000 3440000\n",
      "2023/02/12 22:31:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:31:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:32:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:32:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:32:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3440000 3450000\n",
      "2023/02/12 22:32:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:32:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:32:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:32:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3450000 3460000\n",
      "2023/02/12 22:32:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:32:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:32:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:32:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3460000 3470000\n",
      "2023/02/12 22:32:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:32:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:33:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:33:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:33:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3470000 3480000\n",
      "2023/02/12 22:33:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:33:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:33:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:33:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3480000 3490000\n",
      "2023/02/12 22:33:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:33:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:34:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:34:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:34:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3490000 3500000\n",
      "2023/02/12 22:34:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:34:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:34:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:34:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3500000 3510000\n",
      "2023/02/12 22:34:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:34:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:34:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:34:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3510000 3520000\n",
      "2023/02/12 22:34:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:34:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:35:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:35:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:35:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3520000 3530000\n",
      "2023/02/12 22:35:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:35:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:35:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:35:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3530000 3540000\n",
      "2023/02/12 22:35:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:35:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:36:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:36:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:36:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3540000 3550000\n",
      "2023/02/12 22:36:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:36:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:36:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:36:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3550000 3560000\n",
      "2023/02/12 22:36:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:36:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:36:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:36:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3560000 3570000\n",
      "2023/02/12 22:36:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:36:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:37:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:37:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:37:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3570000 3580000\n",
      "2023/02/12 22:37:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:37:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:37:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:37:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3580000 3590000\n",
      "2023/02/12 22:37:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:37:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:37:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:37:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3590000 3600000\n",
      "2023/02/12 22:37:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:37:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:38:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:38:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:38:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3600000 3610000\n",
      "2023/02/12 22:38:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:38:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:38:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:38:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3610000 3620000\n",
      "2023/02/12 22:38:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:38:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:39:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:39:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:39:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3620000 3630000\n",
      "2023/02/12 22:39:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:39:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:39:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:39:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3630000 3640000\n",
      "2023/02/12 22:39:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:39:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:39:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:39:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3640000 3650000\n",
      "2023/02/12 22:39:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:39:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:40:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:40:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:40:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3650000 3660000\n",
      "2023/02/12 22:40:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:40:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:40:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:40:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3660000 3670000\n",
      "2023/02/12 22:40:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:40:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:40:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:40:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3670000 3680000\n",
      "2023/02/12 22:40:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:40:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:41:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:41:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:41:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3680000 3690000\n",
      "2023/02/12 22:41:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:41:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:41:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:41:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3690000 3700000\n",
      "2023/02/12 22:41:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:41:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:42:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:42:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:42:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3700000 3710000\n",
      "2023/02/12 22:42:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:42:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:42:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:42:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3710000 3720000\n",
      "2023/02/12 22:42:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:42:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:42:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:42:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3720000 3730000\n",
      "2023/02/12 22:42:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:42:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:43:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:43:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:43:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3730000 3740000\n",
      "2023/02/12 22:43:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:43:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:43:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:43:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3740000 3750000\n",
      "2023/02/12 22:43:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:43:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:44:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:44:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:44:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3750000 3760000\n",
      "2023/02/12 22:44:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:44:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:44:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:44:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3760000 3770000\n",
      "2023/02/12 22:44:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:44:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:44:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:44:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3770000 3780000\n",
      "2023/02/12 22:44:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:44:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:45:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:45:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:45:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3780000 3790000\n",
      "2023/02/12 22:45:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:45:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:45:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:45:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3790000 3800000\n",
      "2023/02/12 22:45:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:45:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:45:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:45:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3800000 3810000\n",
      "2023/02/12 22:45:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:45:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:46:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:46:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:46:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3810000 3820000\n",
      "2023/02/12 22:46:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:46:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:46:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:46:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3820000 3830000\n",
      "2023/02/12 22:46:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:46:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:47:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:47:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:47:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3830000 3840000\n",
      "2023/02/12 22:47:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:47:25 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:47:25 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:47:25 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3840000 3850000\n",
      "2023/02/12 22:47:25 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:25 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:25 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:25 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:25 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:47:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:47:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:47:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3850000 3860000\n",
      "2023/02/12 22:47:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:48 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:47:48 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:48:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:48:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:48:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3860000 3870000\n",
      "2023/02/12 22:48:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:48:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:48:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:48:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3870000 3880000\n",
      "2023/02/12 22:48:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:48:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:48:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:48:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3880000 3890000\n",
      "2023/02/12 22:48:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:48:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:49:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:49:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:49:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3890000 3900000\n",
      "2023/02/12 22:49:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:49:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:49:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:49:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3900000 3910000\n",
      "2023/02/12 22:49:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:49:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:50:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:50:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:50:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3910000 3920000\n",
      "2023/02/12 22:50:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:50:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:50:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:50:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3920000 3930000\n",
      "2023/02/12 22:50:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:50:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:50:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:50:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3930000 3940000\n",
      "2023/02/12 22:50:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:50:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:51:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:51:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:51:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3940000 3950000\n",
      "2023/02/12 22:51:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:51:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:51:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:51:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3950000 3960000\n",
      "2023/02/12 22:51:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:51:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:51:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:51:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3960000 3970000\n",
      "2023/02/12 22:51:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:51:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:52:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:52:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:52:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3970000 3980000\n",
      "2023/02/12 22:52:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:52:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:52:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:52:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3980000 3990000\n",
      "2023/02/12 22:52:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:52:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:53:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:53:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:53:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "3990000 4000000\n",
      "2023/02/12 22:53:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:53:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:53:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:53:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4000000 4010000\n",
      "2023/02/12 22:53:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:53:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:53:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:53:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4010000 4020000\n",
      "2023/02/12 22:53:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:53:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:54:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:54:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:54:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4020000 4030000\n",
      "2023/02/12 22:54:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:54:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:54:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:54:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4030000 4040000\n",
      "2023/02/12 22:54:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:54:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:55:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:55:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:55:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4040000 4050000\n",
      "2023/02/12 22:55:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:55:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:55:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:55:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4050000 4060000\n",
      "2023/02/12 22:55:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:55:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:55:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:55:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4060000 4070000\n",
      "2023/02/12 22:55:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:55:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:56:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:56:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:56:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4070000 4080000\n",
      "2023/02/12 22:56:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:56:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:56:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:56:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4080000 4090000\n",
      "2023/02/12 22:56:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:56:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:57:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:57:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:57:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4090000 4100000\n",
      "2023/02/12 22:57:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:57:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:57:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:57:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4100000 4110000\n",
      "2023/02/12 22:57:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:57:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:57:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:57:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4110000 4120000\n",
      "2023/02/12 22:57:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:57:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:58:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:58:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:58:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4120000 4130000\n",
      "2023/02/12 22:58:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:58:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:58:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:58:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4130000 4140000\n",
      "2023/02/12 22:58:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:58:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:58:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:58:57 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4140000 4150000\n",
      "2023/02/12 22:58:57 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:57 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:57 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:58:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:59:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:59:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:59:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4150000 4160000\n",
      "2023/02/12 22:59:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 22:59:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:59:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 22:59:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4160000 4170000\n",
      "2023/02/12 22:59:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 22:59:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:00:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:00:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:00:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4170000 4180000\n",
      "2023/02/12 23:00:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:00:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:00:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:00:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4180000 4190000\n",
      "2023/02/12 23:00:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:00:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:00:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:00:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4190000 4200000\n",
      "2023/02/12 23:00:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:00:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:01:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:01:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:01:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4200000 4210000\n",
      "2023/02/12 23:01:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:01:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:01:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:01:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4210000 4220000\n",
      "2023/02/12 23:01:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:01:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:02:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:02:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:02:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4220000 4230000\n",
      "2023/02/12 23:02:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:02:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:02:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:02:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4230000 4240000\n",
      "2023/02/12 23:02:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:02:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:02:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:02:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4240000 4250000\n",
      "2023/02/12 23:02:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:02:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:03:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:03:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:03:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4250000 4260000\n",
      "2023/02/12 23:03:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:03:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:03:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:03:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4260000 4270000\n",
      "2023/02/12 23:03:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:03:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:03:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:03:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4270000 4280000\n",
      "2023/02/12 23:03:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:03:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:04:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:04:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:04:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4280000 4290000\n",
      "2023/02/12 23:04:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:04:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:04:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:04:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4290000 4300000\n",
      "2023/02/12 23:04:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:04:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:05:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:05:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:05:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4300000 4310000\n",
      "2023/02/12 23:05:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:05:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:05:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:05:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4310000 4320000\n",
      "2023/02/12 23:05:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:05:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:05:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:05:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4320000 4330000\n",
      "2023/02/12 23:05:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:05:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:06:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:06:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:06:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4330000 4340000\n",
      "2023/02/12 23:06:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:06:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:06:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:06:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4340000 4350000\n",
      "2023/02/12 23:06:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:06:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:07:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:07:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:07:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4350000 4360000\n",
      "2023/02/12 23:07:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:07:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:07:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:07:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4360000 4370000\n",
      "2023/02/12 23:07:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:07:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:07:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:07:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4370000 4380000\n",
      "2023/02/12 23:07:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:07:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:08:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:08:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:08:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4380000 4390000\n",
      "2023/02/12 23:08:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:08:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:08:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:08:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4390000 4400000\n",
      "2023/02/12 23:08:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:08:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:08:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:08:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4400000 4410000\n",
      "2023/02/12 23:08:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:08:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:09:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:09:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:09:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4410000 4420000\n",
      "2023/02/12 23:09:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:09:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:09:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:09:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4420000 4430000\n",
      "2023/02/12 23:09:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:09:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:10:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:10:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:10:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4430000 4440000\n",
      "2023/02/12 23:10:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:10:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:10:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:10:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4440000 4450000\n",
      "2023/02/12 23:10:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:10:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:10:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:10:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4450000 4460000\n",
      "2023/02/12 23:10:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:10:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:11:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:11:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:11:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4460000 4470000\n",
      "2023/02/12 23:11:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:11:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:11:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:11:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4470000 4480000\n",
      "2023/02/12 23:11:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:11:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:12:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:12:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:12:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4480000 4490000\n",
      "2023/02/12 23:12:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:12:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:12:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:12:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4490000 4500000\n",
      "2023/02/12 23:12:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:12:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:12:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:12:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4500000 4510000\n",
      "2023/02/12 23:12:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:12:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:13:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:13:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:13:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4510000 4520000\n",
      "2023/02/12 23:13:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:13:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:13:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:13:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4520000 4530000\n",
      "2023/02/12 23:13:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:13:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:13:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:13:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4530000 4540000\n",
      "2023/02/12 23:13:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:13:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:14:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:14:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:14:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4540000 4550000\n",
      "2023/02/12 23:14:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:14:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:14:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:14:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4550000 4560000\n",
      "2023/02/12 23:14:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:14:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:15:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:15:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:15:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4560000 4570000\n",
      "2023/02/12 23:15:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:15:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:15:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:15:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4570000 4580000\n",
      "2023/02/12 23:15:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:15:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:15:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:15:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4580000 4590000\n",
      "2023/02/12 23:15:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:15:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:16:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:16:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:16:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4590000 4600000\n",
      "2023/02/12 23:16:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:16:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:16:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:16:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4600000 4610000\n",
      "2023/02/12 23:16:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:16:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:17:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:17:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:17:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4610000 4620000\n",
      "2023/02/12 23:17:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:17:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:17:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:17:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4620000 4630000\n",
      "2023/02/12 23:17:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:17:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:17:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:17:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4630000 4640000\n",
      "2023/02/12 23:17:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:17:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:18:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:18:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:18:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4640000 4650000\n",
      "2023/02/12 23:18:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:18:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:18:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:18:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4650000 4660000\n",
      "2023/02/12 23:18:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:18:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:18:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:18:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4660000 4670000\n",
      "2023/02/12 23:18:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:18:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:19:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:19:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:19:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4670000 4680000\n",
      "2023/02/12 23:19:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:19:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:19:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:19:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4680000 4690000\n",
      "2023/02/12 23:19:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:19:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:20:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:20:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:20:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4690000 4700000\n",
      "2023/02/12 23:20:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:20:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:20:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:20:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4700000 4710000\n",
      "2023/02/12 23:20:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:20:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:20:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:20:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4710000 4720000\n",
      "2023/02/12 23:20:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:20:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:21:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:21:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:21:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4720000 4730000\n",
      "2023/02/12 23:21:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:21:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:21:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:21:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4730000 4740000\n",
      "2023/02/12 23:21:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:21:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:21:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:21:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4740000 4750000\n",
      "2023/02/12 23:21:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:21:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:22:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:22:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:22:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4750000 4760000\n",
      "2023/02/12 23:22:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:22:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:22:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:22:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4760000 4770000\n",
      "2023/02/12 23:22:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:22:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:23:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:23:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:23:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4770000 4780000\n",
      "2023/02/12 23:23:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:23:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:23:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:23:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4780000 4790000\n",
      "2023/02/12 23:23:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:23:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:23:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:23:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4790000 4800000\n",
      "2023/02/12 23:23:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:23:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:24:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:24:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:24:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4800000 4810000\n",
      "2023/02/12 23:24:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:24:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:24:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:24:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4810000 4820000\n",
      "2023/02/12 23:24:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:24:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:25:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:25:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:25:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4820000 4830000\n",
      "2023/02/12 23:25:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:25:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:25:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:25:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4830000 4840000\n",
      "2023/02/12 23:25:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:25:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:25:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:25:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4840000 4850000\n",
      "2023/02/12 23:25:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:25:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:26:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:26:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:26:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4850000 4860000\n",
      "2023/02/12 23:26:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:26:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:26:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:26:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4860000 4870000\n",
      "2023/02/12 23:26:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:26:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:26:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:26:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4870000 4880000\n",
      "2023/02/12 23:26:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:26:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:27:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:27:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:27:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4880000 4890000\n",
      "2023/02/12 23:27:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:27:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:27:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:27:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4890000 4900000\n",
      "2023/02/12 23:27:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:27:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:28:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:28:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:28:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4900000 4910000\n",
      "2023/02/12 23:28:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:28:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:28:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:28:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4910000 4920000\n",
      "2023/02/12 23:28:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:28:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:28:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:28:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4920000 4930000\n",
      "2023/02/12 23:28:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:28:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:29:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:29:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:29:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4930000 4940000\n",
      "2023/02/12 23:29:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:29:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:29:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:29:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4940000 4950000\n",
      "2023/02/12 23:29:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:29:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:29:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:29:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4950000 4960000\n",
      "2023/02/12 23:29:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:29:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:30:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:30:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:30:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4960000 4970000\n",
      "2023/02/12 23:30:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:30:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:30:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:30:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4970000 4980000\n",
      "2023/02/12 23:30:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:30:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:31:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:31:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:31:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4980000 4990000\n",
      "2023/02/12 23:31:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:31:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:31:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:31:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "4990000 5000000\n",
      "2023/02/12 23:31:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:31:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:31:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:31:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5000000 5010000\n",
      "2023/02/12 23:31:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:31:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:32:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:32:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:32:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5010000 5020000\n",
      "2023/02/12 23:32:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:32:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:32:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:32:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5020000 5030000\n",
      "2023/02/12 23:32:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:32:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:33:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:33:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:33:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5030000 5040000\n",
      "2023/02/12 23:33:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:33:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:33:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:33:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5040000 5050000\n",
      "2023/02/12 23:33:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:33:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:33:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:33:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5050000 5060000\n",
      "2023/02/12 23:33:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:33:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:34:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:34:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:34:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5060000 5070000\n",
      "2023/02/12 23:34:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:34:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:34:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:34:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5070000 5080000\n",
      "2023/02/12 23:34:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:34:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:35:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:35:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:35:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5080000 5090000\n",
      "2023/02/12 23:35:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:35:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:35:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:35:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5090000 5100000\n",
      "2023/02/12 23:35:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:35:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:35:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:35:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5100000 5110000\n",
      "2023/02/12 23:35:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:48 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:35:48 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:36:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:36:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:36:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5110000 5120000\n",
      "2023/02/12 23:36:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:36:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:36:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:36:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5120000 5130000\n",
      "2023/02/12 23:36:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:36:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:36:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:36:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5130000 5140000\n",
      "2023/02/12 23:36:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:36:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:37:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:37:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:37:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5140000 5150000\n",
      "2023/02/12 23:37:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:37:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:37:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:37:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5150000 5160000\n",
      "2023/02/12 23:37:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:37:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:38:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:38:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:38:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5160000 5170000\n",
      "2023/02/12 23:38:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:38:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:38:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:38:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5170000 5180000\n",
      "2023/02/12 23:38:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:38:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:38:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:38:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5180000 5190000\n",
      "2023/02/12 23:38:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:38:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:39:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:39:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:39:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5190000 5200000\n",
      "2023/02/12 23:39:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:39:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:39:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:39:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5200000 5210000\n",
      "2023/02/12 23:39:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:39:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:40:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:40:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:40:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5210000 5220000\n",
      "2023/02/12 23:40:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:40:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:40:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:40:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5220000 5230000\n",
      "2023/02/12 23:40:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:40:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:40:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:40:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5230000 5240000\n",
      "2023/02/12 23:40:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:40:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:41:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:41:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:41:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5240000 5250000\n",
      "2023/02/12 23:41:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:41:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:41:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:41:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5250000 5260000\n",
      "2023/02/12 23:41:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:41:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:42:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:42:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:42:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5260000 5270000\n",
      "2023/02/12 23:42:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:42:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:42:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:42:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5270000 5280000\n",
      "2023/02/12 23:42:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:42:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:42:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:42:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5280000 5290000\n",
      "2023/02/12 23:42:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:42:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:43:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:43:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:43:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5290000 5300000\n",
      "2023/02/12 23:43:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:43:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:43:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:43:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5300000 5310000\n",
      "2023/02/12 23:43:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:43:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:44:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:44:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:44:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5310000 5320000\n",
      "2023/02/12 23:44:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:44:25 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:44:25 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:44:25 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5320000 5330000\n",
      "2023/02/12 23:44:25 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:25 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:25 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:44:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:44:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:44:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5330000 5340000\n",
      "2023/02/12 23:44:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:44:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:45:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:45:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:45:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5340000 5350000\n",
      "2023/02/12 23:45:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:45:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:45:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:45:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5350000 5360000\n",
      "2023/02/12 23:45:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:45:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:45:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:45:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5360000 5370000\n",
      "2023/02/12 23:45:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:45:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:46:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:46:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:46:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5370000 5380000\n",
      "2023/02/12 23:46:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:46:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:46:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:46:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5380000 5390000\n",
      "2023/02/12 23:46:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:46:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:47:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:47:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:47:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5390000 5400000\n",
      "2023/02/12 23:47:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:47:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:47:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:47:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5400000 5410000\n",
      "2023/02/12 23:47:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:47:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:47:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:47:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5410000 5420000\n",
      "2023/02/12 23:47:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:47:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:48:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:48:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:48:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5420000 5430000\n",
      "2023/02/12 23:48:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:48:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:48:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:48:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5430000 5440000\n",
      "2023/02/12 23:48:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:48:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:49:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:49:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:49:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5440000 5450000\n",
      "2023/02/12 23:49:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:49:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:49:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:49:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5450000 5460000\n",
      "2023/02/12 23:49:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:49:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:49:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:49:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5460000 5470000\n",
      "2023/02/12 23:49:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:49:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:50:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:50:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:50:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5470000 5480000\n",
      "2023/02/12 23:50:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:50:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:50:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:50:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5480000 5490000\n",
      "2023/02/12 23:50:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:50:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:51:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:51:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:51:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5490000 5500000\n",
      "2023/02/12 23:51:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:51:25 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:51:25 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:51:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5500000 5510000\n",
      "2023/02/12 23:51:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:51:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:51:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:51:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5510000 5520000\n",
      "2023/02/12 23:51:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:51:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:52:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:52:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:52:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5520000 5530000\n",
      "2023/02/12 23:52:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:52:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:52:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:52:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5530000 5540000\n",
      "2023/02/12 23:52:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:52:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:52:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:52:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5540000 5550000\n",
      "2023/02/12 23:52:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:52:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:53:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:53:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:53:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5550000 5560000\n",
      "2023/02/12 23:53:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:53:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:53:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:53:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5560000 5570000\n",
      "2023/02/12 23:53:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:53:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:54:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:54:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:54:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5570000 5580000\n",
      "2023/02/12 23:54:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:54:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:54:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:54:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5580000 5590000\n",
      "2023/02/12 23:54:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:54:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:54:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:54:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5590000 5600000\n",
      "2023/02/12 23:54:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:54:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:55:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:55:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:55:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5600000 5610000\n",
      "2023/02/12 23:55:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:55:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:55:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:55:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5610000 5620000\n",
      "2023/02/12 23:55:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:55:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:56:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:56:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:56:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5620000 5630000\n",
      "2023/02/12 23:56:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:56:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:56:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:56:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5630000 5640000\n",
      "2023/02/12 23:56:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:56:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:56:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:56:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5640000 5650000\n",
      "2023/02/12 23:56:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:56:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:57:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:57:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:57:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5650000 5660000\n",
      "2023/02/12 23:57:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:57:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:57:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:57:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5660000 5670000\n",
      "2023/02/12 23:57:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:57:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:57:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:57:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5670000 5680000\n",
      "2023/02/12 23:57:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:57:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:58:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:58:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:58:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5680000 5690000\n",
      "2023/02/12 23:58:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:58:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:58:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:58:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5690000 5700000\n",
      "2023/02/12 23:58:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:58:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:59:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:59:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:59:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5700000 5710000\n",
      "2023/02/12 23:59:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:59:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:59:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:59:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5710000 5720000\n",
      "2023/02/12 23:59:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/12 23:59:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:59:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/12 23:59:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5720000 5730000\n",
      "2023/02/12 23:59:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/12 23:59:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:00:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:00:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:00:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5730000 5740000\n",
      "2023/02/13 00:00:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:00:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:00:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:00:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5740000 5750000\n",
      "2023/02/13 00:00:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:00:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:01:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:01:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:01:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5750000 5760000\n",
      "2023/02/13 00:01:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:01:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:01:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:01:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5760000 5770000\n",
      "2023/02/13 00:01:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:01:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:01:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:01:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5770000 5780000\n",
      "2023/02/13 00:01:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:01:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:02:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:02:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:02:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5780000 5790000\n",
      "2023/02/13 00:02:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:02:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:02:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:02:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5790000 5800000\n",
      "2023/02/13 00:02:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:02:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:03:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:03:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:03:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5800000 5810000\n",
      "2023/02/13 00:03:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:03:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:03:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:03:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5810000 5820000\n",
      "2023/02/13 00:03:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:23 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:23 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:03:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:03:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:03:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5820000 5830000\n",
      "2023/02/13 00:03:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:03:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:04:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:04:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:04:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5830000 5840000\n",
      "2023/02/13 00:04:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:04:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:04:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:04:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5840000 5850000\n",
      "2023/02/13 00:04:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:04:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:04:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:04:57 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5850000 5860000\n",
      "2023/02/13 00:04:57 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:57 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:57 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:04:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:05:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:05:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:05:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5860000 5870000\n",
      "2023/02/13 00:05:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:05:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:05:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:05:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5870000 5880000\n",
      "2023/02/13 00:05:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:05:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:06:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:06:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:06:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5880000 5890000\n",
      "2023/02/13 00:06:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:06:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:06:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:06:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5890000 5900000\n",
      "2023/02/13 00:06:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:06:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:06:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:06:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5900000 5910000\n",
      "2023/02/13 00:06:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:06:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:07:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:07:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:07:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5910000 5920000\n",
      "2023/02/13 00:07:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:07:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:07:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:07:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5920000 5930000\n",
      "2023/02/13 00:07:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:07:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:08:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:08:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:08:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5930000 5940000\n",
      "2023/02/13 00:08:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:08:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:08:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:08:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5940000 5950000\n",
      "2023/02/13 00:08:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:08:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:08:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:08:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5950000 5960000\n",
      "2023/02/13 00:08:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:08:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:09:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:09:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:09:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5960000 5970000\n",
      "2023/02/13 00:09:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:09:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:09:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:09:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5970000 5980000\n",
      "2023/02/13 00:09:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:09:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:10:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:10:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:10:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5980000 5990000\n",
      "2023/02/13 00:10:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:10:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:10:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:10:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "5990000 6000000\n",
      "2023/02/13 00:10:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:25 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:25 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:10:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:10:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:10:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6000000 6010000\n",
      "2023/02/13 00:10:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:48 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:10:48 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:11:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:11:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:11:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6010000 6020000\n",
      "2023/02/13 00:11:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:11:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:11:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:11:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6020000 6030000\n",
      "2023/02/13 00:11:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:11:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:11:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:11:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6030000 6040000\n",
      "2023/02/13 00:11:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:11:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:12:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:12:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:12:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6040000 6050000\n",
      "2023/02/13 00:12:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:12:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:12:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:12:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6050000 6060000\n",
      "2023/02/13 00:12:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:12:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:13:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:13:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:13:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6060000 6070000\n",
      "2023/02/13 00:13:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:13:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:13:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:13:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6070000 6080000\n",
      "2023/02/13 00:13:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:13:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:13:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:13:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6080000 6090000\n",
      "2023/02/13 00:13:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:13:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:14:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:14:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:14:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6090000 6100000\n",
      "2023/02/13 00:14:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:14:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:14:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:14:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6100000 6110000\n",
      "2023/02/13 00:14:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:14:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:15:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:15:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:15:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6110000 6120000\n",
      "2023/02/13 00:15:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:15:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:15:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:15:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6120000 6130000\n",
      "2023/02/13 00:15:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:15:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:15:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:15:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6130000 6140000\n",
      "2023/02/13 00:15:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:15:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:16:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:16:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:16:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6140000 6150000\n",
      "2023/02/13 00:16:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:16:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:16:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:16:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6150000 6160000\n",
      "2023/02/13 00:16:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:16:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:17:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:17:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:17:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6160000 6170000\n",
      "2023/02/13 00:17:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:17:25 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:17:25 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:17:25 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6170000 6180000\n",
      "2023/02/13 00:17:25 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:25 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:25 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:17:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:17:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:17:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6180000 6190000\n",
      "2023/02/13 00:17:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:17:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:18:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:18:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:18:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6190000 6200000\n",
      "2023/02/13 00:18:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:18:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:18:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:18:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6200000 6210000\n",
      "2023/02/13 00:18:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:18:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:18:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:18:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6210000 6220000\n",
      "2023/02/13 00:18:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:18:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:19:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:19:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:19:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6220000 6230000\n",
      "2023/02/13 00:19:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:19:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:19:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:19:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6230000 6240000\n",
      "2023/02/13 00:19:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:45 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:19:45 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:20:08 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:20:08 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:20:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6240000 6250000\n",
      "2023/02/13 00:20:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:20:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:20:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:20:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6250000 6260000\n",
      "2023/02/13 00:20:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:20:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:20:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:20:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6260000 6270000\n",
      "2023/02/13 00:20:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:20:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:21:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:21:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:21:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6270000 6280000\n",
      "2023/02/13 00:21:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:21:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:21:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:21:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6280000 6290000\n",
      "2023/02/13 00:21:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:21:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:22:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:22:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:22:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6290000 6300000\n",
      "2023/02/13 00:22:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:22:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:22:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:22:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6300000 6310000\n",
      "2023/02/13 00:22:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:22:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:22:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:22:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6310000 6320000\n",
      "2023/02/13 00:22:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:22:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:23:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:23:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:23:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6320000 6330000\n",
      "2023/02/13 00:23:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:23:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:23:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:23:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6330000 6340000\n",
      "2023/02/13 00:23:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:23:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:24:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:24:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:24:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6340000 6350000\n",
      "2023/02/13 00:24:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:24:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:24:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:24:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6350000 6360000\n",
      "2023/02/13 00:24:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:24:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:24:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:24:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6360000 6370000\n",
      "2023/02/13 00:24:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:24:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:25:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:25:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:25:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6370000 6380000\n",
      "2023/02/13 00:25:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:25:34 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:25:34 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:25:34 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6380000 6390000\n",
      "2023/02/13 00:25:34 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:34 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:34 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:34 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:34 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:25:57 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:25:57 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:25:57 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6390000 6400000\n",
      "2023/02/13 00:25:57 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:57 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:57 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:25:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:26:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:26:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:26:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6400000 6410000\n",
      "2023/02/13 00:26:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:26:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:26:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:26:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6410000 6420000\n",
      "2023/02/13 00:26:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:26:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:27:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:27:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:27:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6420000 6430000\n",
      "2023/02/13 00:27:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:27:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:27:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:27:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6430000 6440000\n",
      "2023/02/13 00:27:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:27:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:27:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:27:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6440000 6450000\n",
      "2023/02/13 00:27:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:27:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:28:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:28:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:28:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6450000 6460000\n",
      "2023/02/13 00:28:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:28:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:28:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:28:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6460000 6470000\n",
      "2023/02/13 00:28:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:41 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:28:41 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:29:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:29:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:29:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6470000 6480000\n",
      "2023/02/13 00:29:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:29:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:29:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:29:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6480000 6490000\n",
      "2023/02/13 00:29:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:29:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:29:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:29:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6490000 6500000\n",
      "2023/02/13 00:29:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:29:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:30:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:30:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:30:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6500000 6510000\n",
      "2023/02/13 00:30:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:30:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:30:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:30:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6510000 6520000\n",
      "2023/02/13 00:30:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:30:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:31:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:31:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:31:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6520000 6530000\n",
      "2023/02/13 00:31:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:31:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:31:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:31:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6530000 6540000\n",
      "2023/02/13 00:31:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:31:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:31:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:31:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6540000 6550000\n",
      "2023/02/13 00:31:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:31:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:32:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:32:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:32:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6550000 6560000\n",
      "2023/02/13 00:32:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:32:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:32:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:32:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6560000 6570000\n",
      "2023/02/13 00:32:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:32:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:32:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:32:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6570000 6580000\n",
      "2023/02/13 00:32:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:32:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:33:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:33:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:33:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6580000 6590000\n",
      "2023/02/13 00:33:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:33:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:33:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:33:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6590000 6600000\n",
      "2023/02/13 00:33:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:33:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:34:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:34:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:34:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6600000 6610000\n",
      "2023/02/13 00:34:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:34:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:34:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:34:28 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6610000 6620000\n",
      "2023/02/13 00:34:28 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:28 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:28 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:34:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:34:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:34:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6620000 6630000\n",
      "2023/02/13 00:34:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:34:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:35:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:35:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:35:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6630000 6640000\n",
      "2023/02/13 00:35:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:35:36 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:35:36 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:35:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6640000 6650000\n",
      "2023/02/13 00:35:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:35:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:35:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:35:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6650000 6660000\n",
      "2023/02/13 00:35:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:35:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:36:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:36:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:36:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6660000 6670000\n",
      "2023/02/13 00:36:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:36:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:36:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:36:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6670000 6680000\n",
      "2023/02/13 00:36:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:36:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:37:06 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:37:06 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:37:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6680000 6690000\n",
      "2023/02/13 00:37:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:37:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:37:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:37:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6690000 6700000\n",
      "2023/02/13 00:37:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:37:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:37:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:37:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6700000 6710000\n",
      "2023/02/13 00:37:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:37:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:38:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:38:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:38:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6710000 6720000\n",
      "2023/02/13 00:38:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:38:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:38:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:38:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6720000 6730000\n",
      "2023/02/13 00:38:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:38:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:39:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:39:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:39:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6730000 6740000\n",
      "2023/02/13 00:39:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:39:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:39:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:39:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6740000 6750000\n",
      "2023/02/13 00:39:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:39:47 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:39:47 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:39:47 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6750000 6760000\n",
      "2023/02/13 00:39:47 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:47 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:47 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:39:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:40:10 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:40:10 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:40:10 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6760000 6770000\n",
      "2023/02/13 00:40:10 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:10 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:10 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:40:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:40:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:40:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6770000 6780000\n",
      "2023/02/13 00:40:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:40:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:40:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:40:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6780000 6790000\n",
      "2023/02/13 00:40:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:40:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:41:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:41:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:41:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6790000 6800000\n",
      "2023/02/13 00:41:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:41:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:41:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:41:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6800000 6810000\n",
      "2023/02/13 00:41:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:41:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:42:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:42:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:42:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6810000 6820000\n",
      "2023/02/13 00:42:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:42:25 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:42:25 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:42:25 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6820000 6830000\n",
      "2023/02/13 00:42:25 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:25 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:25 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:42:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:42:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:42:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6830000 6840000\n",
      "2023/02/13 00:42:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:42:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:43:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:43:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:43:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6840000 6850000\n",
      "2023/02/13 00:43:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:12 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:12 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:43:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:43:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:43:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6850000 6860000\n",
      "2023/02/13 00:43:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:43:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:43:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:43:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6860000 6870000\n",
      "2023/02/13 00:43:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:58 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:43:58 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:44:21 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:44:21 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:44:21 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6870000 6880000\n",
      "2023/02/13 00:44:21 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:21 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:21 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:44:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:44:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:44:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6880000 6890000\n",
      "2023/02/13 00:44:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:44:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:45:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:45:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:45:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6890000 6900000\n",
      "2023/02/13 00:45:07 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:07 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:07 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:07 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:07 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:45:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:45:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:45:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6900000 6910000\n",
      "2023/02/13 00:45:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:45:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:45:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:45:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6910000 6920000\n",
      "2023/02/13 00:45:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:45:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:46:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:46:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:46:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6920000 6930000\n",
      "2023/02/13 00:46:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:46:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:46:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:46:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6930000 6940000\n",
      "2023/02/13 00:46:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:46:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:47:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:47:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:47:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6940000 6950000\n",
      "2023/02/13 00:47:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:47:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:47:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:47:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6950000 6960000\n",
      "2023/02/13 00:47:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:26 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:26 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:47:49 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:47:49 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:47:49 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6960000 6970000\n",
      "2023/02/13 00:47:49 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:49 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:49 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:49 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:47:49 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:48:12 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:48:12 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:48:12 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6970000 6980000\n",
      "2023/02/13 00:48:12 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:12 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:12 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:13 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:13 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:48:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:48:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:48:36 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6980000 6990000\n",
      "2023/02/13 00:48:36 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:36 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:36 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:36 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:36 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:48:59 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:48:59 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:48:59 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "6990000 7000000\n",
      "2023/02/13 00:48:59 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:59 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:59 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:48:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:49:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:49:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:49:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7000000 7010000\n",
      "2023/02/13 00:49:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:49:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:49:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:49:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7010000 7020000\n",
      "2023/02/13 00:49:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:49:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:50:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:50:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:50:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7020000 7030000\n",
      "2023/02/13 00:50:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:50:32 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:50:32 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:50:32 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7030000 7040000\n",
      "2023/02/13 00:50:32 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:32 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:32 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:32 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:50:55 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:50:55 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:50:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7040000 7050000\n",
      "2023/02/13 00:50:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:50:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:51:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:51:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:51:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7050000 7060000\n",
      "2023/02/13 00:51:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:51:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:51:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:51:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7060000 7070000\n",
      "2023/02/13 00:51:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:51:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:52:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:52:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:52:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7070000 7080000\n",
      "2023/02/13 00:52:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:52:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:52:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:52:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7080000 7090000\n",
      "2023/02/13 00:52:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:52:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:52:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:52:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7090000 7100000\n",
      "2023/02/13 00:52:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:52:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:53:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:53:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:53:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7100000 7110000\n",
      "2023/02/13 00:53:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:53:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:53:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:53:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7110000 7120000\n",
      "2023/02/13 00:53:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:53:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:54:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:54:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:54:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7120000 7130000\n",
      "2023/02/13 00:54:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:54:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:54:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:54:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7130000 7140000\n",
      "2023/02/13 00:54:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:54:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:54:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:54:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7140000 7150000\n",
      "2023/02/13 00:54:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:54:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:55:13 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:55:13 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:55:13 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7150000 7160000\n",
      "2023/02/13 00:55:13 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:13 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:13 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:55:37 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:55:37 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:55:37 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7160000 7170000\n",
      "2023/02/13 00:55:37 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:37 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:37 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:37 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:55:37 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:56:00 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:56:00 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:56:00 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7170000 7180000\n",
      "2023/02/13 00:56:00 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:00 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:00 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:00 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:00 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:56:23 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:56:23 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:56:23 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7180000 7190000\n",
      "2023/02/13 00:56:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:24 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:24 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:56:46 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:56:46 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:56:46 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7190000 7200000\n",
      "2023/02/13 00:56:46 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:46 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:46 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:47 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:56:47 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:57:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:57:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:57:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7200000 7210000\n",
      "2023/02/13 00:57:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:10 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:10 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:57:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:57:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:57:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7210000 7220000\n",
      "2023/02/13 00:57:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:57:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:57:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:57:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7220000 7230000\n",
      "2023/02/13 00:57:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:56 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:57:56 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:58:19 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:58:19 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:58:19 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7230000 7240000\n",
      "2023/02/13 00:58:19 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:19 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:19 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:58:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:58:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:58:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7240000 7250000\n",
      "2023/02/13 00:58:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:58:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:59:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:59:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:59:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7250000 7260000\n",
      "2023/02/13 00:59:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:59:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:59:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:59:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7260000 7270000\n",
      "2023/02/13 00:59:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 00:59:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:59:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 00:59:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7270000 7280000\n",
      "2023/02/13 00:59:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 00:59:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:00:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:00:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:00:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7280000 7290000\n",
      "2023/02/13 01:00:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:00:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:00:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:00:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7290000 7300000\n",
      "2023/02/13 01:00:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:00:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:01:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:01:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:01:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7300000 7310000\n",
      "2023/02/13 01:01:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:01:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:01:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:01:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7310000 7320000\n",
      "2023/02/13 01:01:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:01:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:01:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:01:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7320000 7330000\n",
      "2023/02/13 01:01:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:01:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:02:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:02:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:02:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7330000 7340000\n",
      "2023/02/13 01:02:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:02:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:02:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:02:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7340000 7350000\n",
      "2023/02/13 01:02:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:02:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:03:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:03:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:03:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7350000 7360000\n",
      "2023/02/13 01:03:01 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:01 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:01 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:01 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:01 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:03:24 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:03:24 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:03:24 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7360000 7370000\n",
      "2023/02/13 01:03:24 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:24 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:24 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:25 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:25 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:03:48 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:03:48 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:03:48 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7370000 7380000\n",
      "2023/02/13 01:03:48 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:48 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:48 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:48 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:03:48 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:04:11 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:04:11 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:04:11 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7380000 7390000\n",
      "2023/02/13 01:04:11 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:11 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:11 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:11 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:11 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:04:35 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:04:35 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:04:35 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7390000 7400000\n",
      "2023/02/13 01:04:35 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:35 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:35 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:35 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:35 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:04:58 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:04:58 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:04:58 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7400000 7410000\n",
      "2023/02/13 01:04:58 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:58 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:58 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:59 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:04:59 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:05:22 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:05:22 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:05:22 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7410000 7420000\n",
      "2023/02/13 01:05:22 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:22 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:22 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:22 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:22 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:05:45 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:05:45 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:05:45 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7420000 7430000\n",
      "2023/02/13 01:05:45 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:45 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:45 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:46 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:05:46 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:06:09 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:06:09 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:06:09 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7430000 7440000\n",
      "2023/02/13 01:06:09 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:09 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:09 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:09 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:09 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:06:33 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:06:33 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:06:33 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7440000 7450000\n",
      "2023/02/13 01:06:33 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:33 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:33 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:33 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:33 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:06:56 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:06:56 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:06:56 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7450000 7460000\n",
      "2023/02/13 01:06:56 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:56 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:56 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:57 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:06:57 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:07:20 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:07:20 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:07:20 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7460000 7470000\n",
      "2023/02/13 01:07:20 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:20 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:20 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:21 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:21 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:07:44 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:07:44 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:07:44 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7470000 7480000\n",
      "2023/02/13 01:07:44 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:44 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:44 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:44 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:07:44 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:08:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:08:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:08:08 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7480000 7490000\n",
      "2023/02/13 01:08:08 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:08 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:08 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:08 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:08 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:08:31 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:08:31 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:08:31 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7490000 7500000\n",
      "2023/02/13 01:08:31 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:31 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:31 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:31 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:31 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:08:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:08:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:08:55 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7500000 7510000\n",
      "2023/02/13 01:08:55 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:55 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:55 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:55 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:08:55 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:09:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:09:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:09:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7510000 7520000\n",
      "2023/02/13 01:09:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:18 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:18 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:09:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:09:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:09:42 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7520000 7530000\n",
      "2023/02/13 01:09:42 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:42 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:42 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:09:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:10:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:10:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:10:05 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7530000 7540000\n",
      "2023/02/13 01:10:05 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:05 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:05 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:10:29 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:10:29 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:10:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7540000 7550000\n",
      "2023/02/13 01:10:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:10:53 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:10:53 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:10:53 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7550000 7560000\n",
      "2023/02/13 01:10:53 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:53 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:53 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:10:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:11:16 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:11:16 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:11:16 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7560000 7570000\n",
      "2023/02/13 01:11:16 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:16 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:16 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:11:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:11:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:11:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7570000 7580000\n",
      "2023/02/13 01:11:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:11:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:12:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:12:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:12:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7580000 7590000\n",
      "2023/02/13 01:12:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:04 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:04 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:12:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:12:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:12:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7590000 7600000\n",
      "2023/02/13 01:12:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:28 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:28 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:12:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:12:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:12:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7600000 7610000\n",
      "2023/02/13 01:12:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:12:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:13:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:13:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:13:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7610000 7620000\n",
      "2023/02/13 01:13:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:13:39 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:13:39 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:13:39 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7620000 7630000\n",
      "2023/02/13 01:13:39 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:39 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:39 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:13:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:14:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:14:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:14:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7630000 7640000\n",
      "2023/02/13 01:14:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:14:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:14:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:14:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7640000 7650000\n",
      "2023/02/13 01:14:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:14:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:14:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:14:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7650000 7660000\n",
      "2023/02/13 01:14:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:14:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:15:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:15:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:15:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7660000 7670000\n",
      "2023/02/13 01:15:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:15 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:15 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:15:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:15:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:15:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7670000 7680000\n",
      "2023/02/13 01:15:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:15:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:16:02 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:16:02 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:16:02 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7680000 7690000\n",
      "2023/02/13 01:16:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:16:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:16:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:16:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7690000 7700000\n",
      "2023/02/13 01:16:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:16:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:16:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:16:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7700000 7710000\n",
      "2023/02/13 01:16:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:51 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:16:51 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:17:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:17:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:17:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7710000 7720000\n",
      "2023/02/13 01:17:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:17:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:17:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:17:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7720000 7730000\n",
      "2023/02/13 01:17:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:38 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:17:38 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:18:01 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:18:01 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:18:01 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7730000 7740000\n",
      "2023/02/13 01:18:02 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:02 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:02 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:02 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:02 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:18:26 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:18:26 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:18:26 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7740000 7750000\n",
      "2023/02/13 01:18:26 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:26 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:26 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:18:50 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:18:50 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:18:50 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7750000 7760000\n",
      "2023/02/13 01:18:50 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:50 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:50 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:50 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:18:50 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:19:14 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:19:14 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:19:14 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7760000 7770000\n",
      "2023/02/13 01:19:14 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:14 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:14 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:14 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:14 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:19:38 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:19:38 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:19:38 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7770000 7780000\n",
      "2023/02/13 01:19:38 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:38 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:38 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:39 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:19:39 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:20:03 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:20:03 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:20:03 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7780000 7790000\n",
      "2023/02/13 01:20:03 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:03 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:03 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:03 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:03 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:20:27 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:20:27 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:20:27 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7790000 7800000\n",
      "2023/02/13 01:20:27 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:27 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:27 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:27 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:27 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:20:51 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:20:51 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:20:51 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7800000 7810000\n",
      "2023/02/13 01:20:51 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:51 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:51 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:52 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:20:52 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:21:15 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:21:15 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:21:15 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7810000 7820000\n",
      "2023/02/13 01:21:15 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:15 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:15 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:16 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:16 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:21:40 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:21:40 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:21:40 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7820000 7830000\n",
      "2023/02/13 01:21:40 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:40 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:40 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:40 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:21:40 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:22:04 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:22:04 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:22:04 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7830000 7840000\n",
      "2023/02/13 01:22:04 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:04 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:04 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:05 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:05 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:22:28 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:22:28 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:22:29 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7840000 7850000\n",
      "2023/02/13 01:22:29 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:29 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:29 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:29 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:29 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:22:52 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:22:52 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:22:52 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7850000 7860000\n",
      "2023/02/13 01:22:52 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:52 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:52 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:53 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:22:53 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:23:17 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:23:17 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:23:17 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7860000 7870000\n",
      "2023/02/13 01:23:17 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:17 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:17 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:17 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:17 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:23:41 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:23:41 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:23:41 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7870000 7880000\n",
      "2023/02/13 01:23:41 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:41 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:41 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:42 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:23:42 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:24:05 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:24:05 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:24:06 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7880000 7890000\n",
      "2023/02/13 01:24:06 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:06 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:06 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:06 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:06 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:24:30 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:24:30 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:24:30 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7890000 7900000\n",
      "2023/02/13 01:24:30 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:30 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:30 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:30 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:30 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:24:54 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:24:54 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:24:54 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7900000 7910000\n",
      "2023/02/13 01:24:54 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:54 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:54 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:54 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:24:54 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:25:18 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:25:18 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:25:18 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7910000 7920000\n",
      "2023/02/13 01:25:18 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:18 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:18 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:19 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:19 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:25:42 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:25:42 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:25:43 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "7920000 7930000\n",
      "2023/02/13 01:25:43 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:43 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:43 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:43 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=False, filterpos=['名詞', '動詞'], use_orgform=True)\n",
      "2023/02/13 01:25:43 INFO Start to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "[DEBUG] train updately\n",
      "2023/02/13 01:26:07 INFO End to fit n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:26:07 INFO Start to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n",
      "2023/02/13 01:26:07 INFO End to transform n_step=1 model=VectorizerWord2vec(vector_size=128, sg=1, max_vocab_size=1000000, min_count=1, window=7, epochs=5)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: around 5 hours (303m 54.5s)\n",
    "bs = 10000\n",
    "for idx, offset in enumerate(range(0, len(tidy_sentences), bs)):\n",
    "    X = tidy_sentences[offset:offset+bs]\n",
    "    print(offset, offset+bs)\n",
    "    pipe_wikivec.fit([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/pipe_wikivec.retrained.gz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: around 2m = 1m55.0s\n",
    "joblib.dump(pipe_wikivec, \"data/pipe_wikivec.retrained.gz\", compress=(\"gzip\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.component.models.pipeline import Pipeline\n",
    "# from app.component.models.vectorizer import VectorizerWord2vec\n",
    "# from app.domain.models.tokenizer import TokenizerWord\n",
    "# \n",
    "# pipe_vectorizer = Pipeline(\n",
    "#     steps=[\n",
    "#         (\n",
    "#             TokenizerWord(\n",
    "#                 use_stoppoes=False, filterpos=[\"名詞\", \"動詞\"], use_orgform=True\n",
    "#             ),\n",
    "#             None,\n",
    "#         ),\n",
    "#         (VectorizerWord2vec(\n",
    "#             vector_size=128,\n",
    "#             sg=1,\n",
    "#             max_vocab_size=1000 * 1000,\n",
    "#             min_count=1,\n",
    "#             window=7,\n",
    "#             epochs=50,\n",
    "#         ), None),\n",
    "#     ]\n",
    "# )\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tidy_sentences[:100]\n",
    "# pipe_vectorizer.fit([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = pipe_vectorizer.get_model(0)\n",
    "# vectorizer = pipe_vectorizer.get_model(-1)\n",
    "# tokenizer, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = tokenizer([X])\n",
    "# v = vectorizer(words)\n",
    "# # [vectorizer.model.wv[w] for s in words for w in s][0][:10]\n",
    "# v[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = tidy_sentences[len(X):len(X)+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     v2 = pipe_vectorizer([X2])\n",
    "# except Exception as e:\n",
    "#     print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training updately\n",
    "# pipe_vectorizer.fit([X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_vectorizer([X2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
