{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピックへの自動ラベリング\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンテンツを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n \\n\\n\\n \\n\\n砲撃受けたウクライナ東部の露軍臨時兵舎、死者８９人に…露側「原因」は兵士が携帯使用と主張 : 読売新聞オンライン\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://ledge.ai/authorinterview-book-bert-int/\"\n",
    "url = \"https://www.yomiuri.co.jp/world/20230104-OYT1T50056/\"\n",
    "# url = \"https://www3.nhk.or.jp/kansai-news/20230104/2000069636.html\"\n",
    "# url = \"https://news.yahoo.co.jp/articles/fad0c4f41d46b686e0566bf10e4c016a641a9dab\"\n",
    "# url = \"https://news.yahoo.co.jp/articles/4d2d14fd1ca1dc9b134c5f493896a7e30fc1e781\"\n",
    "res = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(res.content, \"lxml\")\n",
    "for tg in [\"script\", \"noscript\", \"meta\"]:\n",
    "    try:\n",
    "        soup.find(tg).replace_with(\" \")\n",
    "    except:\n",
    "        pass\n",
    "soup.get_text()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡易クレンジング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['砲撃受けたウクライナ東部の露軍臨時兵舎、死者８９人に…露側「原因」は兵士が携帯使用と主張 : 読売新聞オンライン',\n",
       " 'すべて',\n",
       " 'ログイン・登録',\n",
       " 'ログイン',\n",
       " '新規登録']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    contents = []\n",
    "    for txt in re.split(r\"(。|\\n)\", text):\n",
    "        txt = txt.strip().replace(\"\\u200b\", \"\").replace(\"\\u3000\", \" \")\n",
    "        txt = re.sub(r\"\\n+\", \"\\n\", txt)\n",
    "        txt = re.sub(r\"([\\W])\\1+\", \" \", txt)\n",
    "        if not txt:\n",
    "            continue\n",
    "        # contents.append(txt)\n",
    "        # contents.extend(txt.split(\"\\n\"))\n",
    "        contents.append(txt.split(\"\\n\")[-1])\n",
    "    return contents\n",
    "\n",
    "text = soup.get_text()\n",
    "contents = clean_text(text)\n",
    "contents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pickup wiki data to analyze topic\n",
    "# wdb = WikiDb(mode=\"train\")\n",
    "# records = wdb.select_by_document_id(document_ids=[\n",
    "#     \"Doc01GN7P2K02Y2FC8BXHMZ5V1A34\",    # 南部煎餅\n",
    "#     \"Doc01GN7P3C1ZPVGB8AXJC75M9ZQ9\",    # 龍が如く OF THE END\n",
    "#     \"Doc01GN7P3C0YPAWH4KT16P7NNQBK\",    # 自動運転車\n",
    "#     \"Doc01GN7P3BS2JNYS3HYV0RM1TT7T\",    # デュケイン大学\n",
    "#     \"Doc01GN7P3BK88YCXKS64H6G2HK0Y\",    # 深蒸し茶\n",
    "#     ])\n",
    "# X: TextSequences = [WikiRecord(*rec).paragraph.splitlines() for rec in records]\n",
    "# pipe_topic.fit(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トピックモデルのパイプラインを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from app.component.models.model import TextSequences\n",
    "from app.component.models.pipeline import Pipeline\n",
    "from app.component.models.vectorizer import VectorizerBoW, VectorizerWord2vec\n",
    "from app.domain.models.tokenizer import TokenizerWord\n",
    "from app.domain.models.topic_model import TopicModel\n",
    "from app.infra.wikidb import WikiDb, WikiRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_topic = Pipeline(\n",
    "    steps=[\n",
    "        (TokenizerWord(use_stoppoes=True), None),\n",
    "        (VectorizerBoW(), None),\n",
    "        (TopicModel(n_topic=10, n_epoch=2000), None),\n",
    "    ],\n",
    "    name=\"pipe_topic_sample\",\n",
    "    do_print=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023/01/08 20:03:23 INFO Start to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=True, filterpos=[], use_orgform=False)\n",
      "2023/01/08 20:03:23 INFO End to fit n_step=0 model=JpTokenizerMeCab(use_stoppoes=True, filterpos=[], use_orgform=False)\n",
      "2023/01/08 20:03:23 INFO Start to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=True, filterpos=[], use_orgform=False)\n",
      "2023/01/08 20:03:32 INFO End to transform n_step=0 model=JpTokenizerMeCab(use_stoppoes=True, filterpos=[], use_orgform=False)\n",
      "2023/01/08 20:03:32 INFO Start to fit n_step=1 model=VectorizerBoW()\n",
      "2023/01/08 20:03:32 INFO End to fit n_step=1 model=VectorizerBoW()\n",
      "2023/01/08 20:03:32 INFO Start to transform n_step=1 model=VectorizerBoW()\n",
      "2023/01/08 20:03:32 INFO End to transform n_step=1 model=VectorizerBoW()\n",
      "2023/01/08 20:03:32 INFO Start to fit n_step=2 model=TopicModel(n_topic=10, n_epoch=2000)\n",
      "2023/01/08 20:03:32 INFO End to fit n_step=2 model=TopicModel(n_topic=10, n_epoch=2000)\n",
      "2023/01/08 20:03:32 INFO Start to transform n_step=2 model=TopicModel(n_topic=10, n_epoch=2000)\n",
      "2023/01/08 20:03:32 INFO End to transform n_step=2 model=TopicModel(n_topic=10, n_epoch=2000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[(JpTokenizerMeCab(use_stoppoes=True, filterpos=[], use_orgform=False), None), (VectorizerBoW(), None), (TopicModel(n_topic=10, n_epoch=2000), None)], name=pipe_topic_sample, do_print=True, args=(), kwargs={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X: TextSequences = [[snt] for snt in contents]\n",
    "X: TextSequences = [contents]\n",
    "# pipe_topic[:1].fit(X).transform(X)        # for debugging\n",
    "pipe_topic.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VectorizerBoW(), TopicModel(n_topic=10, n_epoch=2000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bow: VectorizerBoW = pipe_topic.get_model(1)\n",
    "model_topic: TopicModel = pipe_topic.get_model(-1)\n",
    "model_bow, model_topic\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec モデルをロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use original pretrained newsvec model (with only noun and verb tokens)\n",
    "pipe_newsvec = joblib.load(\"data/pipe_newsvec.gz\")\n",
    "pipe_newsvec\n",
    "model_vectorizer: VectorizerWord2vec = pipe_newsvec.get_model(-1)\n",
    "model_vectorizer\n",
    "w2v = model_vectorizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use original pretrained wikivec model (with only noun and verb tokens)\n",
    "# pipe_wikivec = joblib.load(\"data/pipe_wikivec.noun-verb.gz\")\n",
    "# pipe_wikivec\n",
    "# model_vectorizer: VectorizerWord2vec = pipe_wikivec.get_model(-1)\n",
    "# model_vectorizer\n",
    "# w2v = model_vectorizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use shiroyagi's word2vec pretrained model\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "# w2v = Word2Vec.load(\"data/word2vec.gensim.model\")\n",
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.0000001 , 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 0.99999994, 1.        , 1.0000001 , 1.0000001 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各トピックの確率の合計が 1 になることを確認しておく\n",
    "model_topic.get_topic_probabilities().sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickup_topic_words(model_topic: TopicModel, topn: int = -1):\n",
    "    # topn: 各トピックの上位の単語の数\n",
    "    topics = []\n",
    "    for topic_probs in model_topic.get_topic_probabilities():\n",
    "        indices = topic_probs.argsort()[::-1][:topn]\n",
    "        topic = [(model_bow.vocab[idx], topic_probs[idx]) for idx in indices]\n",
    "        topics.append(topic)\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BERT', 0.03100991),\n",
       " ('し', 0.024096573),\n",
       " ('自然言語処理', 0.022761282),\n",
       " ('AI', 0.022514433),\n",
       " ('の', 0.015532182)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickup_topic_words(model_topic, topn=30)[0][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topic(topics: list):\n",
    "    words = []\n",
    "    probs = []\n",
    "    for w, p in topics:\n",
    "        do_skip: bool = False\n",
    "        do_skip |= bool(re.search(r\"^[あ-ん]\", w))    # です、ます などは、スキップ\n",
    "        do_skip |= bool(re.search(r\"[あ-ん]$\", w))    # 動名詞 などは、スキップ\n",
    "        do_skip |= w[0].isnumeric()                  # 数字から始まるラベルはスキップ\n",
    "        if do_skip:\n",
    "            continue\n",
    "        words.append(w)\n",
    "        probs.append(p)\n",
    "    return words, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import re\n",
    "\n",
    "\n",
    "def estimate_topic_label(w2v, topics: list):\n",
    "    proper_topics = {}\n",
    "    topic_labels = []\n",
    "\n",
    "    for idx_tpc, tpc in enumerate(topics):\n",
    "        _words, _probs = parse_topic(tpc)\n",
    "        words = [w for w in _words if w in w2v.wv]\n",
    "        probs = [p for w, p in zip(_words, _probs) if w in w2v.wv]\n",
    "        vectors = w2v.wv[words]\n",
    "        probs = numpy.array(probs).reshape(-1, 1)\n",
    "        topic_vector = (vectors * probs).sum(axis=0)    # 期待値ベクトル\n",
    "        estimated_topic_labels = w2v.wv.similar_by_vector(topic_vector, topn=100)\n",
    "\n",
    "        labels, similarities = parse_topic(estimated_topic_labels)\n",
    "        topic_label = labels[0]\n",
    "        similarity = similarities[0]\n",
    "\n",
    "        if topic_label not in proper_topics:\n",
    "            proper_topics[topic_label] = (idx_tpc, similarity, words, probs)\n",
    "        topic_labels.append((topic_label, similarity))\n",
    "\n",
    "    return topic_labels, proper_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possibly failed to specify the topic numbers\n"
     ]
    }
   ],
   "source": [
    "# w2v : is already loaded\n",
    "n_topic_words = 17      # to calculate the average over\n",
    "n_topic = 10            # default topic numbers\n",
    "topic_label_counter = {}\n",
    "while True:\n",
    "    pipe_topic = Pipeline(\n",
    "        steps=[\n",
    "            (TokenizerWord(use_stoppoes=True, use_orgform=True), None),\n",
    "            (VectorizerBoW(), None),\n",
    "            (TopicModel(n_topic=n_topic, n_epoch=2000), None),\n",
    "        ],\n",
    "        name=\"pipe_topic\",\n",
    "        do_print=False,\n",
    "    )\n",
    "\n",
    "    X: TextSequences = [contents]\n",
    "    pipe_topic.fit(X)\n",
    "\n",
    "    model_topic: TopicModel = pipe_topic.get_model(-1)\n",
    "    topics = pickup_topic_words(model_topic, topn=n_topic_words)\n",
    "    topic_labels, proper_topics = estimate_topic_label(w2v, topics)\n",
    "\n",
    "    # ラベルをカウント\n",
    "    for tpc in proper_topics:\n",
    "        cnt = topic_label_counter.get(tpc, 0) + 1\n",
    "        topic_label_counter[tpc] = cnt\n",
    "    \n",
    "    # ループの終了条件\n",
    "    if len(proper_topics) >= n_topic:\n",
    "        break\n",
    "    if n_topic <= 2:\n",
    "        print(\"possibly failed to specify the topic numbers\")\n",
    "        break\n",
    "\n",
    "    # 状態/処理文脈を更新\n",
    "    n_topic = len(proper_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "topic[0]: Jabberwacky : 3 (0.781)\n",
      "     ... ['し', 'BERT', 'AI', '社', 'いっ', 'たち', 'ない', 'コーパス', '技術', 'ある程度', '新しい', '統一', 'くれる', '視聴者', '人々', '画像', '感じ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "topic[1]: Jabberwacky : 3 (0.784)\n",
      "     ... ['し', 'AI', 'BERT', '社', 'いっ', 'ある程度', 'ない', '新しい', 'コーパス', '人々', '視聴者', '技術', 'くれる', 'たち', '以前', '感じ', 'はじめて']\n"
     ]
    }
   ],
   "source": [
    "for idx_tpc, (lbl, sim) in enumerate(topic_labels):\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"topic[{idx_tpc}]: {lbl} : {topic_label_counter[lbl]} ({sim:0.3f})\")\n",
    "    print(\" \" * 4 + f\" ... {[_t for _t, _s in topics[idx_tpc][:20]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated n_topic: 1\n",
      "[00]: Jabberwacky: ['AI', '社', 'コーパス', '技術', '統一', '視聴者', '人々', '画像']\n"
     ]
    }
   ],
   "source": [
    "print(\"estimated n_topic:\", len(proper_topics))\n",
    "\n",
    "# print(\"label: (topic index, similarity to the expectation vector, words, probabilities)\")\n",
    "\n",
    "# 実際に期待値ベクトルを算出するときに使った単語を可視化\n",
    "for lbl, v in proper_topics.items():\n",
    "    tpc_idx, sim, words, probs = v\n",
    "    print(f\"[{tpc_idx:02d}]: {lbl}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語でBERTが動くイメージを知ってほしい。\n",
      "『BERTによる自然言語処理入門』の著者に聞く | Ledge.ai AI（人工知能）関連メディア メールマガジン登録 情報提供はこちら About Contact Hot Topics 注目のキーワード Fore- 見えない変化を可視化する。\n",
      "見えている世界が動き出す。\n",
      "Dataiku DATAFLUCT キカガク AI開発の現場 DataRobot JDLA DL for DX 沖電気工業 Gravio 簡単IoTとAI実装 ビジネス 学習 開発 エンタメ ラジオ Podcast 特集：新型コロナとデータ分析 特集：それ私が作りました！ Article Theme テーマ別カテゴリ プレスリリース イベント レッジのAI開発・コンサルティング AI・データ活用研修 AI（人工知能）ニュース インタビュー イベントレポート AI活用事例 やってみた 文系大学生がAI講座受けてみた ナレッジ AI本書評 ツール紹介 サービス紹介 マーケティング マーケティングオートメーション（MA） PR TIMES イベント告知 AI TALK NIGHT Ledge.ai Webinar 海外AIニュース速報 AI（人工知能）ニュースまとめ Technology 技術別カテゴリ ノーコード データサイエンス DX プログラミング エッジAI 自動運転 AI（人工知能） 注目テクノロジー 自然言語処理 画像認識・生成 チャットボット 音声認識・合成 予測 About Contact TOP > Article Theme > インタビュー > 日本語でBERTが動くイメージを知ってほしい。\n",
      "『BERTによる自然言語処理入門』の著者に聞く 2021 08 25 Wed インタビュー 日本語でBERTが動くイメージを知ってほしい。\n",
      "『BERTによる自然言語処理入門』の著者に聞く Tweet 6月に販売開始した、ストックマーク株式会社の機械学習エンジニアによる共著『BERTによる自然言語処理入門：Transformersを使った実践プログラミング』（オーム社）。\n",
      "売れ行きも好調で、発売日の前から重版が決定していた。\n",
      "本書はデータセットの処理から、BERTのファインチューニング（BERTを特定の言語タスクに特化させるための学習）、性能の評価までの一連の流れを、文章分類・固有表現抽出・文章校正・類似文章検索といった言語タスクごとに体験できる。\n",
      " BERTによる自然言語処理入門 Transformersを使った実践プログラミング | Ohmsha「入門」と銘打っているが、はじめて自然言語処理を体験する人や、非エンジニアでも読めるのか？本書の魅力をさらにフカボリすべく、執筆をリードした、ストックマーク株式会社 R&D マネージャーの近江 崇宏氏に話を聞いた。\n",
      "近江 崇宏氏 プロフィールストックマーク株式会社にて自然言語処理の研究開発に従事。\n",
      "2012年に京都大学大学院理学研究科博士課程修了。\n",
      "博士（理学）。\n",
      "その後は、2020年まで東京大学生産技術研究所（最終職位：特任准教授）にて時系列解析を中心とする統計学・機械学習の研究に従事。\n",
      "2020年4月より現職。\n",
      "本のコンセプトは「BERTでできることを知る」ストックマークは、ユーザーの閲覧履歴をAIが学習し、世界中のビジネスニュースの中から業務に直結するニュースをレコメンドする「Anews」をはじめ、自然言語処理技術を生かしたサービスを提供している。\n",
      "テックブログやイベントでのBERT、自然言語処理の情報発信もさかんだ。\n",
      "日本語の自然言語処理ノウハウを持つそんな同社が、なぜ入門にフォーカスしたのか？そこには”入門書”の方が売れる、というわずかなたくらみだけでなく、BERTを使って自然言語処理のいろいろなタスクができることを知ってほしい、という執筆チームの強い思いがあった。\n",
      " 近江「BERTでどのようなことができるのかのイメージを掴んでもらえればと思っています。\n",
      "私も専門外の分野、例えば画像処理でいろいろなモデルの名前を聞くことはありますが、何ができるかというのはあまり知りません。\n",
      "そうした、機械学習をやっているけれど自然言語処理は未経験という方や、BERTで何ができるか分からない、という方に向けた『入門』としています」実際に本書では、BERTとBERTを扱うためのライブラリであるTransformersの概要や使い方を解説したのち、自然言語処理の各タスクの説明に入る。\n",
      "目次には「固有表現抽出」「文書校正」「類似文章」といった章が並び、読者は本文を読み進める前から、BERTを使って何ができるかを知ることができる。\n",
      "機械学習の経験がなくてもBERTを使えるワケこの本には同類の解説書と違うふたつの特徴がある。\n",
      "そのひとつが、BERTやTransformersの使い方に着目した章立てだ。\n",
      "BERTやTransformersの使い方も解説まず本書の前半で自然言語処理やBERTについて解説があり、「自然言語処理とは？」「ニューラル言語モデルとは？」といった基礎的な部分も、コンパクトかつていねいな説明で紐解く。\n",
      "後半では、Transformersという事前学習済み言語モデルを提供するライブラリを用いて、BERTを動かして言語タスクを解く。\n",
      "BERTとTransformersとの組み合わせも近江氏いわく「自然言語処理ではスタンダードな構成」であるものの、初学者にとってはハードルが高い。\n",
      " 近江「Transformersの使い方は、インターネットで調べれば解説記事が見つかります。\n",
      "ただ公式ドキュメントや一般のブログも、”見る人が見れば分かる”というもので、ある程度の前提知識がないと理解をするのは難しい。\n",
      "特にTransformersの公式ドキュメントはすべて英語なので、私自身が初めて勉強したときも分かりづらいなと思っていたのです。\n",
      "本書では、あまり使ったことがない人でも動かせるように、丁寧な解説を心がけました」ライブラリの基本的な使い方は巻末の「付録」などで紹介する技術書が多い中、本書では初学者を想定した解説がなされている。\n",
      "英語データセットで分かるのはBERTが動いているという事実だけこの本でもうひとつ特徴的なのが、日本語データセットを使った解説だ。\n",
      "自然言語処理の解説書やオンラインの記事では、著者が日本人でも英語のデータセットを使って解説することが少なくない。\n",
      "日本語のデータセットの数は英語などに比べて少ないという事情があるからだ。\n",
      "近江氏は執筆プロジェクトが立ち上がったとき「日本語のデータセットで統一しましょう」と提案したほど、強いこだわりを持っていたのだとか。\n",
      " 近江「極端な話、BERTが動いているかどうかを知るには英語のデータセットでも問題ありません。\n",
      "でも英語になじみがないと”動いている”ことがわかるだけですし、ちょっと面白くないですよね。\n",
      "自然言語処理は私たちの日常に密接に関わっています。\n",
      "多くの日本人が日本語の言葉や文章のイメージを自分なりに持っているので、日本語のデータセットの方が、読者の方に楽しんでいただけるのではないかと考えました」本書では大量のデータを事前学習させたBERTを使うほか、「Wikipedia を用いた日本語の固有表現抽出データセット」や「日本語Wikipedia 入力誤りデータセット」、「livedoor ニュースコーパス」、「chABSAデータセット」など自然言語処理の各タスクに適した公開データセットをファインチューニング用に用いる。\n",
      "日本語データセットを使うからこそ、直感的に「自然な文章が出た」「文章のネガポジ判定に成功した」などの処理結果が分かるのだ。\n",
      "実は過去に経験がなかった「文章校正（９章）」。\n",
      "手を動かしながら執筆を進めた技術書の執筆というと、著者がこれまで培った専門知識や経験・ノウハウを集約して出すものが多いが、自分の知識がアップデートされるということも少なくない。\n",
      "近江氏も本書の執筆を進めていく中で、新たな学びがあったとのこと。\n",
      "9章の「文章校正」は、執筆担当の森長 誠氏と近江氏は共に経験がなく、まさに手探りでの挑戦だったそうだ。\n",
      "担当編集者にも「できるんですか？」と心配されたほど。\n",
      "それでも試行錯誤を経て、なんとか本書で解説できる形まで持っていったという。\n",
      " 近江「最初はうまく行くのか自信がありませんでしたが、データセットをどう使うのか、どういうふうにしたら読者にとって分かりやすいのか、などと考えながら手を動かしました。\n",
      "当初の想定よりも完成度が高いものができて、新しい学びも多くありました。\n",
      "逆に、8章の『固有表現抽出』は、私が業務で培った経験が特に活かせたように思います。\n",
      "書きたいことが書けたという実感がありましたので、こちらもぜひ見ていただけると嬉しいです」固有表現抽出とは、文章から人名・組織名といった固有名詞を抽出するタスクのこと。\n",
      "本書では解説に加え、同氏がプロダクト開発で培ったノウハウも充実している。\n",
      "まずは、BERTを動かしてみてほしいタスクごとに拾い読みができる構成の本ではあるが、読者におすすめしたい読み方はあるか？を聞いてみると、 近江「自然言語処理に詳しくない方は、BERTを知らない方は1章から3章まで。\n",
      "どうやって動くかに興味を持った人は4〜6章ぐらいまでを、まずしっかり読んでみてください。\n",
      "SNSで『後半部分は難しい』という声がちらほらありましたが、7章から10章は、処理の内容が複雑になり、コードもそれ以前の章に比べると難しくなります。\n",
      "ただし、すべてを理解しなければ次の章に進めない、というものではないので、まずはざっくりとイメージを掴んでもらえたらと思います」とのことだ。\n",
      "本書は一般書と専門書の間のレベルに調節していて、読者のレベル感も幅広く想定している。\n",
      "読み進めていくと難しい部分もあるかもしれないが、ブラウザ上から実行できるGoogle Colaboratoryで気軽に試してみてほしいそうだ。\n",
      " 近江「本書で、BERTで何ができるのか、どういう仕組みで動いているのか、というイメージを持っていただけたら嬉しいかなというところです。\n",
      "モデルやアルゴリズムの詳細な解説もできなくはありませんでしたが、実践的な内容にしたいと思い、あえて必要最低限に留めました。\n",
      "実際に手を動かして、BERTで何ができるか、どう使うかを理解できる内容になっていると思います。\n",
      "前半のモデルの解説を難しく感じられたら、まずは後半のコードを動かすところでイメージを掴むのも良いかもしれません。\n",
      "未経験者でもどんどん実践してみてほしい。\n",
      "このBERT入門をきっかけに、自然言語処理そのものにも興味を持っていただけたら嬉しいです」先日、Ledge.ai Webinar vol.32『自然言語処理でビジネスパーソンが注目すべき技術とは？BERT以降の変遷とこれから』に登壇。\n",
      "多くの視聴者が注目した当分はもう書きたくない？けど これまでの話でうかがったとおり、本書は入門書という位置づけだ。\n",
      "本書の続編や違った切り口での発信を考えているか？とたずねたところ、直近で続編を出す予定はないが、今後もBERTや自然言語処理などの情報発信は続けていきたいという。\n",
      " 近江「執筆はハードスケジュールでしたし、今の段階ではもう書きたくないという気持ちもありますが（笑）もう少し体力が戻ってきたらできるかもしれないかなといったところです。\n",
      "どんなトピックを取り上げるかはまだ未定ですが、我々としては最先端の技術を一般に広く、わかりやすく伝えていきたいと考えています。\n",
      "プロダクト開発で新しい技術も取り入れていっているので、新しい技術でどういうことができるかは今後も発信していきたいと思っています」自然言語処理の”はじめの一歩”を後押しする一冊インタビュー中も一貫して「まずはBERTを触って試してほしい」と近江氏が言っていたとおり、非エンジニアの筆者が本書を手にとって最初に感じたのが、圧倒的なとっつきやすさだ。\n",
      "BERTやTransformersの解説ページをはじめ、ブラウザ（Google Colaboratory）中心の環境構築、日本語データセットが「とりあえず触ってみる」まで、自然言語処理を始めるうえで背中を後押ししてくれる。\n",
      "この本を参考に、先日筆者も文章の穴埋めタスクを体験してみた。\n",
      "初めて自分のマシンでBERTが動いて、日本語の文章が画面に表示されたのを見たときは、感動すらおぼえた。\n",
      "コードが詳しく分からなくても、英語に慣れていなくても、自然言語処理の”はじめの一歩”を踏み出せる。\n",
      "『BERTによる自然言語処理入門：Transformersを使った実践プログラミング』は、以下リンクなどで現在販売中だ。\n",
      "本の詳細、購入はこちらから関連記事：日本語データセットで学べるBERT入門書『BERTによる自然言語処理入門』6月28日に販売開始 by 菊田 千春 雑誌記者、IT企業のオウンドメディア運営・記事制作を経てレッジに。\n",
      "新しいものに目がなく、テクノロジーの進化による人々の行動様式・価値観の変化を探っていくのが好き。\n",
      "Tag AI（人工知能）, 自然言語処理, インタビュー, 学習 Share Tweet 関連する記事 2023 01 04 Wed リズムゲームの譜面制作時間をAIで加速。\n",
      "人工知能分野の国際学会「AAAI-23」で採択 2022 12 30 Fri 学生・社会人ともにPythonが人気 Paizaが「プログラミング言語に関する調査（2022年度版）」の結果を発表 2022 12 30 Fri 欲しいイラストをいつでも生成可能 テキストからいらすとや風の画像を生成する「AIいらすとや」がリリース 2022 12 27 Tue NVIDIAのCEOの完全AI駆動型アバターToy Jensen 「ジングルベル」でホリデーシーズンを盛り上げる 2022 12 27 Tue 【年内限定公開】無料ウェビナー｜TRYETHINGが語る、ノーコード予測AI『ウムベルト』 2022 12 27 Tue 【年内限定公開】無料ウェビナー｜日本最大級の医療プラットフォームが考える『エンジニアに必要な視点』とは？ 検索: よく読まれている記事 【アーカイブ配信中】菱洋エレクトロ・フツパー・FastLabel 3社と語る『AI導入を成功へ導いたリアル手法 3選』 AI時代に必要な「なめらかさ」とは？ AIの専門家たちがAI開発のヒントと未来を語り合った「AI TALK NIGHT sponsored by Gravio」レポート 理系大学生が徹底解説！ Gravioを使って「俺か、俺以外か」を自動で判断・可視化するAIシステムを構築 プログラム未経験の大学生でも旅行写真認識AIを構築できた！写真に映っている名所旧跡の名前をLINEに通知するシステムも作ってみた NVIDIA、滋賀大学と連携しデータサイエンス教材の日本語版を無償で提供開始 注目のキーワード Fore- 見えない変化を可視化する。\n",
      "見えている世界が動き出す。\n",
      "Dataiku DATAFLUCT キカガク AI開発の現場 DataRobot JDLA DL for DX 沖電気工業 Gravio 簡単IoTとAI実装 ビジネス 学習 開発 エンタメ ラジオ Podcast 特集：新型コロナとデータ分析 特集：それ私が作りました！ Follow Us!     Share Tweet Copyright © 2023 Ledge inc. All Rights Reserved.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(contents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 703 is out of bounds for axis 1 with size 703",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lda \u001b[39m=\u001b[39m model_topic\u001b[39m.\u001b[39mmodel\n\u001b[1;32m      2\u001b[0m bow \u001b[39m=\u001b[39m pipe_topic[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](X)\n\u001b[0;32m----> 3\u001b[0m vis \u001b[39m=\u001b[39m pyLDAvis\u001b[39m.\u001b[39;49mgensim_models\u001b[39m.\u001b[39;49mprepare(lda, corpus\u001b[39m=\u001b[39;49mbow, dictionary\u001b[39m=\u001b[39;49mmodel_bow\u001b[39m.\u001b[39;49mvocab)\n\u001b[1;32m      4\u001b[0m pyLDAvis\u001b[39m.\u001b[39mdisplay(vis)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyLDAvis/gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     78\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39mprepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyLDAvis/gensim_models.py:69\u001b[0m, in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     67\u001b[0m     topic \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mget_lambda()\n\u001b[1;32m     68\u001b[0m topic \u001b[39m=\u001b[39m topic \u001b[39m/\u001b[39m topic\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m---> 69\u001b[0m topic_term_dists \u001b[39m=\u001b[39m topic[:, fnames_argsort]\n\u001b[1;32m     71\u001b[0m \u001b[39massert\u001b[39;00m topic_term_dists\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m doc_topic_dists\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mtopic_term_dists\u001b[39m\u001b[39m'\u001b[39m: topic_term_dists, \u001b[39m'\u001b[39m\u001b[39mdoc_topic_dists\u001b[39m\u001b[39m'\u001b[39m: doc_topic_dists,\n\u001b[1;32m     74\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdoc_lengths\u001b[39m\u001b[39m'\u001b[39m: doc_lengths, \u001b[39m'\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m'\u001b[39m: vocab, \u001b[39m'\u001b[39m\u001b[39mterm_frequency\u001b[39m\u001b[39m'\u001b[39m: term_freqs}\n",
      "\u001b[0;31mIndexError\u001b[0m: index 703 is out of bounds for axis 1 with size 703"
     ]
    }
   ],
   "source": [
    "lda = model_topic.model\n",
    "bow = pipe_topic[:-1](X)\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus=bow, dictionary=model_bow.vocab)\n",
    "pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
