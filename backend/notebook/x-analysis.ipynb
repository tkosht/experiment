{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../data/tweet_activity_metrics_tkosht_20240106_20240203_ja.csv\"\n",
    "df_posts = pd.read_csv(csv_file)\n",
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file = \"../data/x_refs.tsv\"\n",
    "df_refs = pd.read_csv(tsv_file, sep=\"\\t\", header=0)\n",
    "df_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_posts, df_refs], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\"url\": \"参照URL\", \"text\": \"参照テキスト\"}, axis=1).fillna(\"(NULL)\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ツイート本文\"].to_list()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-small\")\n",
    "# embedder = AutoModel.from_pretrained(\"intfloat/multilingual-e5-small\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\")\n",
    "# embedder = AutoModel.from_pretrained(\"intfloat/multilingual-e5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "embedder = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "\n",
    "\n",
    "def embed(\n",
    "    embedder: BertModel,\n",
    "    input_texts=list[str],\n",
    "    do_normalize: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    batch_dict = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "    embedder.eval()\n",
    "    embedder.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedder(**batch_dict)\n",
    "        embeddings = average_pool(\n",
    "            outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "        )\n",
    "        if do_normalize:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_post = embed(\n",
    "    embedder, input_texts=df[\"ツイート本文\"].to_list(), device=torch.device(\"cuda:1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ref = embed(\n",
    "    embedder, input_texts=df[\"参照テキスト\"].to_list(), device=torch.device(\"cuda:1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = embedder.embeddings.word_embeddings.weight\n",
    "# with torch.no_grad():\n",
    "#     D = embeddings_post @ W.T\n",
    "# tokenizer.decode(torch.argmax(D, dim=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = F.normalize(embeddings_post) @ F.normalize(embeddings_ref).T\n",
    "sin = torch.sqrt(1 - cos**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linalg.norm(embeddings_post, dim=1)\n",
    "b = torch.linalg.norm(embeddings_ref, dim=1)\n",
    "\n",
    "# # NOTE: scores: 対応する2つのベクトルが成す三角形の面積\n",
    "# scores = (1 / 2) * a * b * sin\n",
    "\n",
    "# NOTE: 扇形の面積\n",
    "scores = torch.sqrt(a * b) * torch.arccos(cos) / torch.pi\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_array = torch.diag(scores).cpu().numpy()\n",
    "score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(\"number\").drop(\"ツイートID\", axis=1).dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[\n",
    "    [\n",
    "        \"インプレッション\",\n",
    "        \"エンゲージメント\",\n",
    "        \"リツイート\",\n",
    "        \"いいね\",\n",
    "        \"ユーザープロフィールクリック\",\n",
    "        \"URLクリック数\",\n",
    "        \"詳細クリック\",\n",
    "        \"score\",\n",
    "    ]\n",
    "].corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cmap = sns.color_palette(\"coolwarm\", 200)\n",
    "sns.heatmap(df_corr, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"インプレッション\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ((df[\"インプレッション\"] / 10).round() * 10).mode()\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df[\"インプレッション\"].median()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: クラス分類にする\n",
    "df[\"high_impression\"] = (df[\"インプレッション\"] > m).astype(int)\n",
    "df[\"high_impression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "X = F.normalize(embeddings_post).cpu().numpy()\n",
    "y = df[[\"high_impression\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def run_cross_validation(X, y, params: dict, n_splits: int = 5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = []\n",
    "    for train_indices, valid_indices in kf.split(X):\n",
    "        X_train, X_valid = X[train_indices], X[valid_indices]\n",
    "        y_train, y_valid = y[train_indices], y[valid_indices]\n",
    "\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        p = model.predict(X_valid)\n",
    "        score = log_loss(y_valid, p)\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=5,\n",
    "    num_trees=256,\n",
    "    num_threads=8,\n",
    "    # max_depth=8,\n",
    "    # min_data_in_leaf=0,\n",
    "    min_samples_in_leaf=3,\n",
    "    # min_sum_hessian_in_leaf=100,\n",
    "    random_state=seed,\n",
    ")\n",
    "scores = run_cross_validation(X, y, params)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fixed_params = dict(\n",
    "    num_threads=8,\n",
    "    min_samples_in_leaf=3,\n",
    "    # min_data_in_leaf=0,\n",
    "    # min_sum_hessian_in_leaf=100,\n",
    "    random_state=seed,\n",
    "    objective=\"binary\",\n",
    "    metric=\"binary_logloss\",\n",
    ")\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def objective(self, trial: optuna.Trial):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            self.X,\n",
    "            self.y,\n",
    "            test_size=0.3,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "        trainset = lgb.Dataset(X_train, label=y_train)\n",
    "        params = dict(\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "            num_leaves=trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "            num_trees=trial.suggest_int(\"num_trees\", 10, 1000),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 4, 5),\n",
    "            lambda_l1=trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            lambda_l2=trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "            feature_fraction=trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "            bagging_fraction=trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "            bagging_freq=trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 40),\n",
    "        )\n",
    "        params.update(fixed_params)\n",
    "\n",
    "        model = lgb.train(params=params, train_set=trainset)\n",
    "        p_rates = model.predict(X_valid)\n",
    "        p = np.rint(p_rates)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_valid, p)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evl = Evaluator(X, y)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(evl.objective, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization\n",
    "\n",
    "\n",
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = lgb.Dataset(data=X_train, label=y_train)\n",
    "validset = lgb.Dataset(data=X_valid, label=y_valid, reference=trainset)\n",
    "params = study.best_params.copy()\n",
    "params.update(fixed_params)\n",
    "\n",
    "evals = {}\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=trainset,\n",
    "    valid_sets=[trainset, validset],\n",
    "    valid_names=[\"trainset\", \"validset\"],\n",
    "    callbacks=[lgb.record_evaluation(evals)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_metric(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ツイート本文\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"high_impression\"]].replace(1, \"high_impression\").replace(\n",
    "    0, \"low_impression\"\n",
    ").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"ツイート本文\"]].to_numpy()\n",
    "y = (\n",
    "    df[[\"high_impression\"]]\n",
    "    .replace(1, \"high_impression\")\n",
    "    .replace(0, \"low_impression\")\n",
    "    .to_numpy()\n",
    "    .ravel()\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from sklearn.decomposition import PCA  # , KernelPCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from app.morph.classify import (\n",
    "    JpTokenizerJanome,\n",
    "    JpTokenizer,\n",
    "    ident_tokener,\n",
    "    Transer,\n",
    "    SparsetoDense,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipleline_with_tfidf(tokener: JpTokenizer, n_classes: int, params: dict = {}):\n",
    "    tfidf = TfidfVectorizer(tokenizer=ident_tokener, lowercase=False)\n",
    "\n",
    "    n_components = params.pop(\"n_components\", 2)\n",
    "    embedders = [\n",
    "        (\"pca\", PCA(n_components=n_components)),\n",
    "        (\"identity\", Transer()),  # means tfidf to tfidf\n",
    "    ]\n",
    "\n",
    "    lgbmclf = lightgbm.LGBMClassifier(**params)\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"tokenizer\", tokener),\n",
    "            (\"vectorizer\", tfidf),\n",
    "            (\"to_dence\", SparsetoDense()),\n",
    "            (\"embedder\", FeatureUnion(embedders)),\n",
    "            (\"classifier\", lgbmclf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(set(y_train.ravel().tolist()))\n",
    "is_binary = n_classes == 2\n",
    "\n",
    "fixed_params = dict(\n",
    "    num_threads=8,\n",
    "    min_samples_in_leaf=3,\n",
    "    random_state=seed,\n",
    "    objective=\"binary\" if is_binary else \"softmax\",\n",
    "    metric=\"binary_logloss\" if is_binary else None,\n",
    "    num_class=None if is_binary else n_classes,\n",
    "    importance_type=\"gain\",\n",
    ")\n",
    "\n",
    "\n",
    "class PipelineEvaluator(object):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def objective(self, trial: optuna.Trial):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            self.X,\n",
    "            self.y,\n",
    "            test_size=0.3,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "        params = dict(\n",
    "            n_components=trial.suggest_int(\"n_components\", 2, 64),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "            num_leaves=trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "            num_trees=trial.suggest_int(\"num_trees\", 10, 1000),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 4, 5),\n",
    "            lambda_l1=trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            lambda_l2=trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "            feature_fraction=trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "            bagging_fraction=trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "            bagging_freq=trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 40),\n",
    "        )\n",
    "        params.update(fixed_params)\n",
    "\n",
    "        tokener = JpTokenizerJanome()\n",
    "        n_classes = len(set(y_train.ravel().tolist()))\n",
    "        pipe = build_pipleline_with_tfidf(\n",
    "            tokener=tokener, n_classes=n_classes, params=params\n",
    "        )\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        p = pipe.predict(X_valid)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_valid, p)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pev = PipelineEvaluator(X, y)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(pev.objective, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokener = JpTokenizerJanome()\n",
    "n_classes = len(set(y_train.ravel().tolist()))\n",
    "params = study.best_params.copy()\n",
    "params.update(fixed_params)\n",
    "pipe = build_pipleline_with_tfidf(tokener=tokener, n_classes=n_classes, params=params)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# predict trainset\n",
    "p = pipe.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict validset\n",
    "p = pipe.predict(X_valid)\n",
    "valid_acc = accuracy_score(y_valid, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
