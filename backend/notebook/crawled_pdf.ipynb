{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def extract_and_save_pdf():\n",
    "    # データベースに接続\n",
    "    conn = sqlite3.connect(\"data/crawled_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # content_type が 'pdf' のレコードを1つ取得\n",
    "        cursor.execute(\"SELECT url, content FROM pages WHERE content_type = 'pdf' LIMIT 1\")\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            url, content = result\n",
    "\n",
    "            # ファイル名を生成（URLの最後の部分を使用）\n",
    "            filename = os.path.basename(url)\n",
    "            if not filename.endswith(\".pdf\"):\n",
    "                filename = f\"{filename}_{datetime.now().strftime('%Y%m%d%H%M%S')}.pdf\"\n",
    "\n",
    "            # コンテンツをファイルに書き込む\n",
    "            with open(filename, \"wb\") as file:\n",
    "                file.write(content)\n",
    "\n",
    "            print(f\"PDFファイルが正常に保存されました: {filename}\")\n",
    "        else:\n",
    "            print(\"PDFコンテンツが見つかりませんでした。\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLiteエラーが発生しました: {e}\")\n",
    "    finally:\n",
    "        # 接続を閉じる\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_and_save_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "\n",
    "def html_to_markdown(html_content):\n",
    "    # BeautifulSoupを使用してHTMLを解析\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # html2textコンバーターを初期化\n",
    "    h = html2text.HTML2Text()\n",
    "    h.body_width = 0  # 行の折り返しを無効化\n",
    "    h.unicode_snob = True  # Unicodeを保持\n",
    "    h.skip_internal_links = False  # 内部リンクを保持\n",
    "\n",
    "    # 特定のHTML要素を処理\n",
    "    for tag in soup([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        level = int(tag.name[1])\n",
    "        tag.replace_with(f\"{'#' * level} {tag.text}\\n\\n\")\n",
    "\n",
    "    for tag in soup([\"strong\", \"b\"]):\n",
    "        tag.replace_with(f\"**{tag.text}**\")\n",
    "\n",
    "    for tag in soup([\"em\", \"i\"]):\n",
    "        tag.replace_with(f\"*{tag.text}*\")\n",
    "\n",
    "    for tag in soup.find_all(\"a\", href=True):\n",
    "        tag.replace_with(f\"[{tag.text}]({tag['href']})\")\n",
    "\n",
    "    for tag in soup.find_all(\"img\", src=True):\n",
    "        alt_text = tag.get(\"alt\", \"\")\n",
    "        tag.replace_with(f\"![{alt_text}]({tag['src']})\")\n",
    "\n",
    "    for tag in soup.find_all(\"pre\"):\n",
    "        code = tag.find(\"code\")\n",
    "        if code:\n",
    "            language = code.get(\"class\", [\"\"])[0].replace(\"language-\", \"\")\n",
    "            tag.replace_with(f\"```{language}\\n{code.text}\\n```\\n\\n\")\n",
    "        else:\n",
    "            tag.replace_with(f\"```\\n{tag.text}\\n```\\n\\n\")\n",
    "\n",
    "    for tag in soup.find_all(\"code\"):\n",
    "        if tag.parent.name != \"pre\":\n",
    "            tag.replace_with(f\"`{tag.text}`\")\n",
    "\n",
    "    # テーブルの処理\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        markdown_table = []\n",
    "        headers = []\n",
    "\n",
    "        # ヘッダーの処理\n",
    "        for th in table.find_all(\"th\"):\n",
    "            headers.append(th.text.strip())\n",
    "\n",
    "        if headers:\n",
    "            markdown_table.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "            markdown_table.append(\"| \" + \" | \".join([\"---\" for _ in headers]) + \" |\")\n",
    "\n",
    "        # 行の処理\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all([\"td\", \"th\"])\n",
    "            if cells:\n",
    "                markdown_row = \"| \" + \" | \".join(cell.text.strip() for cell in cells) + \" |\"\n",
    "                markdown_table.append(markdown_row)\n",
    "\n",
    "        # テーブルをMarkdown形式に置き換え\n",
    "        table.replace_with(\"\\n\".join(markdown_table) + \"\\n\\n\")\n",
    "\n",
    "    # 残りのHTMLをMarkdownに変換\n",
    "    markdown = h.handle(str(soup))\n",
    "\n",
    "    # 余分な空白行を削除\n",
    "    markdown = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown)\n",
    "\n",
    "    return markdown.strip()\n",
    "\n",
    "\n",
    "# 使用例\n",
    "html_content = \"\"\"\n",
    "<h1>Welcome to My Website</h1>\n",
    "<p>This is a <strong>bold</strong> and <em>italic</em> text.</p>\n",
    "<h2>Product Comparison</h2>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Product</th>\n",
    "        <th>Price</th>\n",
    "        <th>Rating</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Widget A</td>\n",
    "        <td>$10.99</td>\n",
    "        <td>4.5/5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gadget B</td>\n",
    "        <td>$24.99</td>\n",
    "        <td>4.2/5</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p>Check out our <a href=\"https://example.com/products\">product page</a> for more information.</p>\n",
    "\"\"\"\n",
    "\n",
    "markdown_result = html_to_markdown(html_content)\n",
    "print(markdown_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "import posixpath\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "DEFAULT_PORTS: Dict[str, str] = {\n",
    "    \"http\": \":80\",\n",
    "    \"https\": \":443\",\n",
    "    # 必要に応じて他のスキームも追加\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    URLを正規化する関数。\n",
    "\n",
    "    Args:\n",
    "        url (str): 正規化したいURL。\n",
    "\n",
    "    Returns:\n",
    "        str: 正規化されたURL。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URLをパース\n",
    "        parsed: urllib.parse.ParseResult = urllib.parse.urlparse(url)\n",
    "\n",
    "        # スキームとホストを小文字に\n",
    "        scheme: str = parsed.scheme.lower()\n",
    "        netloc: str = parsed.netloc.lower()\n",
    "\n",
    "        # デフォルトポートを削除\n",
    "        for default_scheme, default_port in DEFAULT_PORTS.items():\n",
    "            if scheme == default_scheme and netloc.endswith(default_port):\n",
    "                netloc = netloc[: -len(default_port)]\n",
    "                break\n",
    "\n",
    "        # パスの正規化\n",
    "        path: str = urllib.parse.unquote(parsed.path)\n",
    "        path = posixpath.normpath(path)\n",
    "        if parsed.path.endswith(\"/\") and not path.endswith(\"/\"):\n",
    "            # 元のパスが末尾スラッシュを含んでいた場合、正規化後もスラッシュを保持\n",
    "            path += \"/\"\n",
    "        # ルートパスでない場合、末尾のスラッシュを削除\n",
    "        if path != \"/\" and path.endswith(\"/\"):\n",
    "            path = path.rstrip(\"/\")\n",
    "        path = urllib.parse.quote(path, safe=\"/\")\n",
    "\n",
    "        # クエリのソート\n",
    "        query_list: List[Tuple[str, str]] = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)\n",
    "        query_list.sort()\n",
    "        query: str = urllib.parse.urlencode(query_list)\n",
    "\n",
    "        # フラグメントを削除\n",
    "        fragment: str = \"\"\n",
    "\n",
    "        # 正規化されたURLを再構築\n",
    "        normalized: str = urllib.parse.urlunparse((scheme, netloc, path, \"\", query, fragment))\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid URL provided: {url}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://emosta.com/en/about -> https://emosta.com/en/about\n",
      "https://emosta.com/en/about/ -> https://emosta.com/en/about\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://emosta.com/en/about\",\n",
    "    \"https://emosta.com/en/about/\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    normalized_url = normalize_url(url)\n",
    "    print(url, \"->\", normalized_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
