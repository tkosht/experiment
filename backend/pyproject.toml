[tool.poetry]
name = "experiment-backend"
version = "0.1.0"
description = "backend experiment project"
authors = ["tkosht"]
license = "MIT"
readme = "README.md"
packages = [{include = "app"}]


[tool.poetry.dependencies]
python = ">=3.10,<3.12"
requests = "*"
beautifulsoup4 = "*"
# python-whois = "*"
# rich = "*"


# for data science
numpy = "*"
pandas = "*"
# fireducks = "*"
# dask = {extras = ["complete"], version = "*"}
scikit-learn = "*"
scipy = "*"
mlxtend = "*"
sympy = "*"
tqdm = "*"
matplotlib = "*"
seaborn = "*"
plotly = "*"
pandasgui = "*"
pandas-profiling = "*"
pygwalker = "*"
# scikit-optimize = "*"
optuna = "*"
lime = "*"
shap = "*"
# pyyaml = "5.3.1"
pyyaml = "*"
# memory_profiler = "*"

# PyQt5 = "=5.15.2"
# lit = "^17.0.2"
# ipynb = "*"

# for experiment management
mlflow = "*"
# tensorboard = "*"
# tensorboardX = "*"
# # python-dateutil==2.8.0
# hydra-core = "*"
# sqlite-web = "*"
evidently = "*"

# crawler
scrapy = "*"
selenium = "*"
webdriver_manager = "*"
retry = "*"
# firecrawl-py = "*"
html2text = "*"


# Database
# psycopg2 = "*"
# psycopg2-binary = "*"

# other tools
omegaconf = "*"
# notebook = "*"
python-dotenv = "*"
ulid-py = "*"
typer = "*"
# simplejson = "*"


# # for project
# fastapi = "*"
# uvicorn = "*"
# # aiofiles = "*"
# # python-multipart = "*"

# for being faster torch dependency resolving
# # torch = {url = "https://download.pytorch.org/whl/cu118/torch-2.2.2%2Bcu118-cp310-cp310-linux_x86_64.whl"}
# # # for installing vllm
# torch = {url = "https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp310-cp310-linux_x86_64.whl"}
# # torchvision = "*"
# # torchvision = {url = "https://download.pytorch.org/whl/cu118/torch-2.2.2%2Bcu118-cp310-cp310-linux_x86_64.whl"}

# torchmetrics = "*"
# nltk = "*"
# torchtext = "*"
# pytorch-forecasting = "*"
# tensorflow-datasets = "*"

# python-snappy = "*"
# pyro-ppl = "*"
# pystan = "2.19.1.1"
# pystan = "*"
# prophet = "*"
# ruptures = "*"
# neo4j = "*"
# neomodel = "*"
# networkx-neo4j = "*"

# # # for NLP
# unidic-lite = "*"
# sudachidict_core = "*"
# sudachidict_small = "*"
# sudachidict_full = "*"
# # spacy = "*"
# # spacy = {extras = ["cuda110"], version = "*"}
# # spacy-transformers = "1.1.3"
# # ja_ginza_electra = "*"
# # fugashi = "*"
# # ipadic = "*"
# # networkx = "*"
# gensim = "*"
# janome = "*"
# mecab-python3 = "*"
# sentencepiece = "*"
# einops = "*"
# # wordcloud = "*"

# # transformers = "*"
# # git+https://github.com/huggingface/transformers
# # transformers = {git = "https://github.com/huggingface/transformers"}
# # transformers = "^4.43.1"
# transformers = "*"
# datasets = "^2.15.0"

# # LLM relatives
# # # for langchain
langchain = "*"
langchain-community = "*"
langchain-openai = "*"
langchain-experimental = "*"
neo4j = "*"
# numexpr = "*"
# langchain = ">=0.0.354"
# langchain-experimental = "*"
# langgraph="*"
# langchain_openai = "*"
# # langflow = "*"
# tavily-python = "*"
# openai = "*"
# google-genai = "*"
# # open-interpreter = "*"
# google-search-results = "*"
# google-api-python-client = "*"
# google-generativeai = "*"
langchain-google-genai = "*"
# pillow = "*"
# chromadb = "*"
# tiktoken = "*"
# sentence_transformers = "*"
# faiss-gpu = "*"
# trafilatura = "*"
# llama-index = "*"
# llama-index-embeddings-huggingface = "*"
# llama-index-embeddings-langchain = "*"
# llama-index-llms-anthropic = "*"
# llama-index-readers-notion = "*"
# llama-index-packs-sentence-window-retriever = "*"
# llama-index-vector-stores-faiss = "*"
# llama-index-postprocessor-rankgpt-rerank = "*"


dify-client = "*"
ragas = "*"
langfuse = "*"
# langsmith = "*"
# promptflow = "*"
# promptflow-tools = "*"


# PDF
pymupdf = "*"
pypdf = "*"
# pymupdf4llm = "*"
# cryptography = "*"
unstructured = {extras = ["pdf"], version = "*"}
# pdfminer.six = "*"
# pycryptodome = "*"
# pandas_datareader = "*"
azure-ai-documentintelligence = "*"


# wikipedia = "*"
# Flask-Cors = "*"

# # guidance
# guidance = "*"

# # Semantic Kernel
# semantic-kernel = "*"

# for LLM development
# codeboxapi = "*"
huggingface_hub = "*"
llama-cpp-python = "*"
# # vllm = "*"
# # Install vLLM with CUDA 11.8.
# vllm = {url="https://github.com/vllm-project/vllm/releases/download/v0.4.0/vllm-0.4.0+cu118-cp310-cp310-manylinux1_x86_64.whl"}
# # heron = {git = "https://github.com/turingmotors/heron.git"}
accelerate = "^0.26.0"


# # for elyza/ELYZA-japanese-Llama-2-7b-instruct
# # transformers
# # datasets
# # mecab-python3
# # unidic-lite
# # sentencepiece
# accelerate = "*"
# bitsandbytes = "*"
# pynvml = "*"
# deepspeed = "*"
# mpi4py = "*"
# peft = "*"
# trl = "*"
# gguf = "^0.1.0"
# exllamav2 = "*"
# fastparquet = "*"

# webapp
gradio = "*"
streamlit = "*"
# streamlit-chat = "*"
# streamlit-extras = "*"
# streamsync = {extras = ["ds"], version = "*"}
# hugchat = "*"

# # datascience
# lightgbm = "*"
# dtw-python = "^1.3.0"
# llvmlite = "^0.40.1"
# numba = "^0.57.1"
# tslearn = "^0.6.1"
# sktime = "^0.21.0"
# # protobuf = "3.20.1"
# protobuf = "^4.24.2"
# jupyter-kernel-gateway = "^2.5.2"
# statsmodels = "^0.14.0"
# yfinance = "^0.2.28"
# pingouin = "^0.5.3"
markitdown = "*"
langchain-anthropic = "*"
anthropic = "*"


[tool.poetry.group.dev.dependencies]
# for sqa
pudb = "*"
pylint = "*"
flake8 = "*"
black = "*"
coverage = "*"
pytest = "*"
pytest-flake8 = "*"
pytest-cov = "*"
pytest-xdist = "*"
pytest-html = "*"
# docutils==0.15
Sphinx = "*"
ipykernel = "^6.29.5"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

