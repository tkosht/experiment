{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia データをロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.auto_topic.infra.wikidb import WikiDb, WikiRecord\n",
    "# from app.auto_topic.component.models.model import TextSequences\n",
    "# \n",
    "# # n_limit: int = -1\n",
    "# n_limit: int = 10\n",
    "# mode: str = \"train\"\n",
    "# wdb = WikiDb(mode=mode)\n",
    "# records = wdb.select(n_limit=n_limit)\n",
    "# X: TextSequences = [WikiRecord(*rec).paragraph.splitlines() for rec in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダウンロードしたニュース記事をロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data2/bk/2/news.json\", \"r\") as f:\n",
    "    news = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34845"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(news)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"ja\" data-service-name=\"kite-article\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, minimum-scale=1\"/><script>LUX=(function(){var a=(\"undefined\"!==typeof(LUX)&&\"undefined\"!==typeof(LUX.gaMarks)?LUX.gaMarks:[]);var d=(\"undefined\"!==typeof(LUX)&&\"undefined\"!==typeof(LUX.gaMeasures)?LUX.gaMeasures:[]);var j=\"LUX_start\";var k=window.performance;var l=(\"undefined\"!==typeof(LUX)&&LUX.ns?LUX.ns:(Date.now?Date.now():+(new Date())));if(k&&k.timing&&k.timing.navigationStart){l=k.timing.navigationStart}function f(){if(k&&k.now){return k.now()}var o=Date.now?Date.now():+(new Date());return o-l}function b(n){if(k){if(k.mark){return k.mark(n)}else{if(k.webkitMark){return k.webkitMark(n)}}}a.push({name:n,entryType:\"mark\",startTime:f(),duration:0});return}function m(p,t,n){if(\"undefined\"===typeof(t)&&h(j)){t=j}if(k){if(k.measure){if(t){if(n){return k.measure(p,t,n)}else{return k.measure(p,t)}}else{return k.measure(p)}}else{'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy\n",
    "# \n",
    "# idx = numpy.random.randint(0, n)\n",
    "# html_text = news[idx][\"html\"]\n",
    "# html_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/sample.html\", \"w\") as f:\n",
    "#     f.write(html_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def _filter_text(body) -> str:\n",
    "    soup: BeautifulSoup = BeautifulSoup(body, \"lxml\")\n",
    "    for tg in [\"script\", \"noscript\", \"meta\"]:\n",
    "        try:\n",
    "            soup.find(tg).replace_with(\" \")\n",
    "        except Exception:\n",
    "            # NOTE: Not Found `tg` tag\n",
    "            pass\n",
    "    return soup.get_text(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n \\n\\n「多島美」めでる展望台\\u3000まさに絵画の世界: 日本経済新聞\\n\\n日本経済新聞\\n\\n朝刊・夕刊\\n\\nLIVE\\n\\nMyニュース\\n\\n日経会社情報\\n\\n人事ウオッチ\\n\\n日経ビジネス\\n\\nお申し込み\\n\\nログイン\\n\\nメニュー\\n\\n検索\\n\\nトップ\\n\\n速報\\n\\nアクセスランキング\\n\\nトピック一覧\\n\\n人事\\n\\nおくやみ\\n\\nプレスリリース\\n\\nビジュアルデータ\\n\\n写真\\n\\n映像\\n\\n社説\\n\\n経済指標・統計\\n\\n連載\\n\\n話題のトピッ'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = _filter_text(html_text)\n",
    "# text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    contents = []\n",
    "    for txt in re.split(r\"(。|\\n)\", text):\n",
    "        txt = txt.strip().replace(\"\\u200b\", \"\").replace(\"\\u3000\", \" \")\n",
    "        txt = re.sub(r\"\\n+\", \"\\n\", txt)\n",
    "        txt = re.sub(r\"([\\W])\\1+\", \" \", txt)\n",
    "        if not txt:\n",
    "            continue\n",
    "        # contents.append(txt)\n",
    "        # contents.extend(txt.split(\"\\n\"))\n",
    "        contents.append(txt.split(\"\\n\")[-1])\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1673"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texts = clean_text(text)\n",
    "# len(texts)\n",
    "# # texts[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cleaned_texts = []\n",
    "for itm in news:\n",
    "    html = itm[\"html\"]\n",
    "    text = _filter_text(html)\n",
    "    texts = clean_text(text)\n",
    "    cleaned_texts.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned_texts.gz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(cleaned_texts, \"cleaned_texts.gz\", compress=(\"gzip\", 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.auto_topic.domain.models.tokenizer import TokenizerWord\n",
    "\n",
    "tokenizer = TokenizerWord(use_stoppoes=False, filterpos=[], use_orgform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_sentences = []\n",
    "for idx, texts in enumerate(cleaned_texts):\n",
    "    for text in texts:\n",
    "        tokenized = tokenizer([[text]])\n",
    "        if not tokenized:\n",
    "            continue\n",
    "        snt = tokenized[0]\n",
    "        if len(snt) < 3:\n",
    "            continue\n",
    "        # print(idx, snt)       # for debugging\n",
    "        tidy_sentences.append(\"\".join(snt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tidy_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tidy_sentences.gz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tidy_sentences, \"tidy_sentences.gz\", compress=(\"gzip\", 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure_sentences = set(tidy_sentences)\n",
    "# len(pure_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n = len(pure_sentences)\n",
    "# # idx = numpy.random.randint(0, n)\n",
    "# list(pure_sentences)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.auto_topic.component.models.pipeline import Pipeline\n",
    "# from app.auto_topic.component.models.vectorizer import VectorizerWord2vec\n",
    "# \n",
    "# pipe_vectorizer = Pipeline(\n",
    "#     steps=[\n",
    "#         (\n",
    "#             TokenizerWord(\n",
    "#                 use_stoppoes=False, filterpos=[\"名詞\", \"動詞\"], use_orgform=True\n",
    "#             ),\n",
    "#             None,\n",
    "#         ),\n",
    "#         (VectorizerWord2vec(\n",
    "#             vector_size=128,\n",
    "#             sg=1,\n",
    "#             max_vocab_size=1000 * 1000,\n",
    "#             min_count=1,\n",
    "#             window=7,\n",
    "#             epochs=50,\n",
    "#         ), None),\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer:VectorizerWord2vec = pipe_vectorizer.get_model(-1)\n",
    "# vectorizer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.auto_topic.component.simple_logger import log_info\n",
    "# \n",
    "# X.extend([[s] for s in pure_sentences])\n",
    "# \n",
    "# log_info(\"Start\", \"Fit Wiki data\")\n",
    "# pipe_vectorizer.fit(X)\n",
    "# log_info(\"End\", \"Fit Wiki data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer:VectorizerWord2vec = pipe_vectorizer.get_model(-1)\n",
    "# len(vectorizer.model.wv.index_to_key)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# \n",
    "# pipe_file = \"data/pipe_newswikivec.gz\"\n",
    "# log_info(\"Start\", \"Dump Pipeline for Wiki vectorizer\")\n",
    "# joblib.dump(pipe_vectorizer, pipe_file, compress=(\"gzip\", 3))\n",
    "# log_info(\"End\", \"Dump Pipeline for Wiki vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
